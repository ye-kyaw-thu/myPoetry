{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building GPT Model with kabyar-Corpus\n",
    "\n",
    "#### Run by Ye Kyaw Thu\n",
    "#### Lab Leader, Language Understanding Lab., Myanmar\n",
    "#### Visiting Professor, NECTEC, Thailand\n",
    "#### Date: 18 April 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Corpus Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/rnd/tool/nanoGPT'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rnd/tool/nanoGPT/data/kabyar_char\n"
     ]
    }
   ],
   "source": [
    "%cd data/kabyar_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  50941  110342 3462210 kabyar-corpus-ver1.0.txt\n"
     ]
    }
   ],
   "source": [
    "!wc kabyar-corpus-ver1.0.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: တက်လူ့တေးသံ\n",
      "By: ဇော်ဂျီ\n",
      "ကြက်ဖ သာလျှင်\n",
      "အာရုဏ်ရောင်လှ ၊ ဝင်းဝါကြ၏ ။\n",
      "ဥဩ သာလျှင်\n",
      "ရာသီနွေလ ၊ ဖူးပွင့်ကြ၏ ။\n",
      "ဖားငယ် သာလျှင်\n",
      "အာကာမိုးက ၊ မိုးရွာကြ၏ ။\n",
      "တက်လူ သာလျှင်\n",
      "မြန်မာပြည်လှ ၊ အားသစ်ရ၍\n",
      "ဇေယျအောင်လံ ထူမည်တည်း ။\n",
      "\n",
      "Title: ဤနေရာ\n",
      "By: ဇော်ဂျီ\n",
      "ဤနေရာတွင်\n",
      "ညောင်ညိုပင်၏ ၊ မြေပြင်ခြေရင်း\n",
      "မြစ်ပါးပျဉ်းသည် ၊ သက်ဆင်းလူးလွန့်\n",
      "မြွေသို့တွန့်၏ ။\n",
      "ဤနေရာတွင်\n",
      "ပိန္နဲ့ပင်ဝယ် ၊ ရှဉ့်ရင်ပေါ့ပါး\n",
      "မြီးဖားဖားသည် ၊ ရွက်ကြားခက်လက်\n",
      "လျှပ်သို့လက်၏ ။\n",
      "ဤနေရာတွင်\n",
      "ထန်းနှစ်ပင်သည် ၊ တူယှဉ်ပြိုင်မြင့်\n",
      "ရွက်ဝန်းဖွင့်၍ ၊ အကျင့်သိက္ခာ\n",
      "ရှင်သို့သာတည့် ။\n",
      "ဤနေရာတွင်\n",
      "စေတီရှင်သည် ၊ ဖြူစင်မောက်မို့\n",
      "ကြားဖူးသို့တည့် ။\n",
      "ဤနေရာတွင်\n"
     ]
    }
   ],
   "source": [
    "!head -n 30 ./kabyar-corpus-ver1.0.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocab Building and Training/Validation Data Separation\n",
    "\n",
    "I assigned kabyar-corpus filename and the updated python script is as follows:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"\n",
      "Prepare the Shakespeare dataset for character-level language modeling.\n",
      "So instead of encoding with GPT-2 BPE tokens, we just map characters to ints.\n",
      "Will save train.bin, val.bin containing the ids, and meta.pkl containing the\n",
      "encoder and decoder and some other related info.\n",
      "\"\"\"\n",
      "import os\n",
      "import pickle\n",
      "import requests\n",
      "import numpy as np\n",
      "\n",
      "# download the tiny shakespeare dataset\n",
      "input_file_path = os.path.join(os.path.dirname(__file__), 'kabyar-corpus-ver1.0.txt')\n",
      "#if not os.path.exists(input_file_path):\n",
      "#    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
      "#    with open(input_file_path, 'w') as f:\n",
      "#        f.write(requests.get(data_url).text)\n",
      "\n",
      "with open(input_file_path, 'r') as f:\n",
      "    data = f.read()\n",
      "print(f\"length of dataset in characters: {len(data):,}\")\n",
      "\n",
      "# get all the unique characters that occur in this text\n",
      "chars = sorted(list(set(data)))\n",
      "vocab_size = len(chars)\n",
      "print(\"all the unique characters:\", ''.join(chars))\n",
      "print(f\"vocab size: {vocab_size:,}\")\n",
      "\n",
      "# create a mapping from characters to integers\n",
      "stoi = { ch:i for i,ch in enumerate(chars) }\n",
      "itos = { i:ch for i,ch in enumerate(chars) }\n",
      "def encode(s):\n",
      "    return [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
      "def decode(l):\n",
      "    return ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
      "\n",
      "# create the train and test splits\n",
      "n = len(data)\n",
      "train_data = data[:int(n*0.9)]\n",
      "val_data = data[int(n*0.9):]\n",
      "\n",
      "# encode both to integers\n",
      "train_ids = encode(train_data)\n",
      "val_ids = encode(val_data)\n",
      "print(f\"train has {len(train_ids):,} tokens\")\n",
      "print(f\"val has {len(val_ids):,} tokens\")\n",
      "\n",
      "# export to bin files\n",
      "train_ids = np.array(train_ids, dtype=np.uint16)\n",
      "val_ids = np.array(val_ids, dtype=np.uint16)\n",
      "train_ids.tofile(os.path.join(os.path.dirname(__file__), 'train.bin'))\n",
      "val_ids.tofile(os.path.join(os.path.dirname(__file__), 'val.bin'))\n",
      "\n",
      "# save the meta information as well, to help us encode/decode later\n",
      "meta = {\n",
      "    'vocab_size': vocab_size,\n",
      "    'itos': itos,\n",
      "    'stoi': stoi,\n",
      "}\n",
      "with open(os.path.join(os.path.dirname(__file__), 'meta.pkl'), 'wb') as f:\n",
      "    pickle.dump(meta, f)\n",
      "\n",
      "# length of dataset in characters:  1115394\n",
      "# all the unique characters:\n",
      "#  !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "# vocab size: 65\n",
      "# train has 1003854 tokens\n",
      "# val has 111540 tokens\n",
      "\n",
      "#length of dataset in characters: 2,788,790\n",
      "#all the unique characters: \n",
      "# %&'*+.0123456789:;=@ABCDEFGHIJKLMNOPQRSTUVWXYZ_abcdefghijklmnopqrstuvwxyz°äæèêíöāīōšūɑɛʔʾʿˈˌːΙίαγδηλνοςόंगजणतपभयरसहािु्கஙசபரிுூ்กคจชณทธนพมยรศสอะัาีุไကခဂဃငစဆဇဈဉညဋဌဍဏတထဒဓနပဖဗဘမယရလဝသဟဠအဣဤဥဦဧဩဪါာိီုူေဲံ့း္်ျြွှဿ၀၁၂၃၄၅၆၇၈၉၌၍၎၏ṃṇṯệ​‌‘’“”−いおさじたつなにのみんビマミャルンー人代加南和坡島得新日昭時本甸缅語﻿\n",
      "#vocab size: 269\n",
      "#train has 2,509,911 tokens\n",
      "#val has 278,879 tokens\n"
     ]
    }
   ],
   "source": [
    "!cat ./prepare-my-char.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the above python script ...**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters: 1,247,440\n",
      "all the unique characters: \n",
      " !\"&'(),-./0137:=?ABCDEFGHIKLMNOPRSTUVWY_`abcdefghijklmnoprstuvwxyကခဂဃငစဆဇဈဉညဋဌဍဎဏတထဒဓနပဖဗဘမယရလဝသဟဠအဣဤဥဦဧဩဪါာိီုူေဲံ့း္်ျြွှဿ၀၁၂၃၄၅၆၇၈၉၊။၌၍၎၏႕​‌‘’“”…﻿\n",
      "vocab size: 151\n",
      "train has 1,122,696 tokens\n",
      "val has 124,744 tokens\n"
     ]
    }
   ],
   "source": [
    "!python ./prepare-my-char.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the splitted training, validation output files:**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kabyar-corpus-ver1.0.txt  meta.pkl  prepare-my-char.py\ttrain.bin  val.bin\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Configuration File for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-kabyar-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 200\n",
      "log_interval = 10 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'kabyar-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'kabyar_char'\n",
      "batch_size = 64\n",
      "block_size = 256 # context of up to 256 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 6\n",
      "n_head = 6\n",
      "n_embd = 384\n",
      "dropout = 0.2\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 5000\n",
      "lr_decay_iters = 5000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n"
     ]
    }
   ],
   "source": [
    "!cat /home/rnd/tool/nanoGPT/config/train_kabyar_char.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the GPU Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 18 03:12:16 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.182.03   Driver Version: 470.182.03   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:0A:00.0 Off |                  N/A |\n",
      "| 30%   45C    P0    58W / 300W |      0MiB / 11019MiB |      1%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:42:00.0 Off |                  N/A |\n",
      "| 61%   69C    P0    71W / 257W |      0MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  Off  | 00000000:43:00.0 Off |                  N/A |\n",
      "| 20%   64C    P0    73W / 250W |      0MiB / 11016MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training GPT-2 Model with Kabyar Character Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rnd/tool/nanoGPT\n"
     ]
    }
   ],
   "source": [
    "%cd /home/rnd/tool/nanoGPT/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rnd/anaconda3/envs/nanoGPT/lib/python3.8/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use-env is set by default in torchrun.\n",
      "If your script expects `--local-rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  warnings.warn(\n",
      "Overriding config with ./config/train_kabyar_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-kabyar-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 200\n",
      "log_interval = 10 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'kabyar-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'kabyar_char'\n",
      "batch_size = 64\n",
      "block_size = 256 # context of up to 256 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 6\n",
      "n_head = 6\n",
      "n_embd = 384\n",
      "dropout = 0.2\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 5000\n",
      "lr_decay_iters = 5000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "total number of tokens per iteration: 81920\n",
      "found vocab_size = 151 (inside data/kabyar_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 10.68M\n",
      "using fused AdamW: True\n",
      "compiling the model... (takes a ~minute)\n",
      "step 0: train loss 5.1298, val loss 5.1309\n",
      "[2023-04-18 03:24:29,993] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-18 03:24:30,924] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-18 03:24:31,745] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-18 03:24:32,080] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-18 03:24:32,399] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-18 03:24:32,730] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-18 03:24:33,050] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-18 03:24:33,380] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-18 03:24:33,700] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-18 03:24:34,030] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-18 03:24:34,351] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-18 03:24:34,679] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "iter 0: loss 5.1263, time 28762.96ms, mfu -100.00%\n",
      "iter 10: loss 3.8088, time 266.87ms, mfu 7.00%\n",
      "iter 20: loss 3.0802, time 266.82ms, mfu 7.00%\n",
      "iter 30: loss 2.7525, time 268.29ms, mfu 7.00%\n",
      "iter 40: loss 2.5939, time 268.48ms, mfu 6.99%\n",
      "iter 50: loss 2.5453, time 267.92ms, mfu 6.99%\n",
      "iter 60: loss 2.4876, time 268.21ms, mfu 6.99%\n",
      "iter 70: loss 2.4880, time 269.09ms, mfu 6.98%\n",
      "iter 80: loss 2.4637, time 268.99ms, mfu 6.98%\n",
      "iter 90: loss 2.4309, time 269.58ms, mfu 6.98%\n",
      "iter 100: loss 2.4581, time 270.13ms, mfu 6.97%\n",
      "iter 110: loss 2.4620, time 270.34ms, mfu 6.96%\n",
      "iter 120: loss 2.3890, time 270.40ms, mfu 6.96%\n",
      "iter 130: loss 2.3738, time 270.06ms, mfu 6.95%\n",
      "iter 140: loss 2.3212, time 271.18ms, mfu 6.95%\n",
      "iter 150: loss 2.2788, time 271.56ms, mfu 6.94%\n",
      "iter 160: loss 2.2281, time 270.77ms, mfu 6.94%\n",
      "iter 170: loss 2.1963, time 271.78ms, mfu 6.93%\n",
      "iter 180: loss 2.1536, time 271.70ms, mfu 6.93%\n",
      "iter 190: loss 2.1414, time 272.62ms, mfu 6.92%\n",
      "iter 200: loss 2.1042, time 273.27ms, mfu 6.91%\n",
      "iter 210: loss 2.0657, time 272.95ms, mfu 6.90%\n",
      "iter 220: loss 2.0760, time 277.48ms, mfu 6.89%\n",
      "iter 230: loss 1.9991, time 273.38ms, mfu 6.88%\n",
      "iter 240: loss 1.9981, time 275.50ms, mfu 6.87%\n",
      "step 250: train loss 1.9318, val loss 1.9981\n",
      "saving checkpoint to out-kabyar-char\n",
      "iter 250: loss 1.9858, time 10126.61ms, mfu 6.20%\n",
      "iter 260: loss 1.9409, time 274.58ms, mfu 6.26%\n",
      "iter 270: loss 1.9304, time 278.09ms, mfu 6.31%\n",
      "iter 280: loss 1.9527, time 272.86ms, mfu 6.36%\n",
      "iter 290: loss 1.9207, time 271.67ms, mfu 6.41%\n",
      "iter 300: loss 1.9270, time 272.14ms, mfu 6.46%\n",
      "iter 310: loss 1.8934, time 271.74ms, mfu 6.50%\n",
      "iter 320: loss 1.9105, time 272.34ms, mfu 6.54%\n",
      "iter 330: loss 1.9227, time 273.52ms, mfu 6.57%\n",
      "iter 340: loss 1.8752, time 272.54ms, mfu 6.59%\n",
      "iter 350: loss 1.8489, time 274.98ms, mfu 6.61%\n",
      "iter 360: loss 1.8351, time 275.60ms, mfu 6.63%\n",
      "iter 370: loss 1.8350, time 272.84ms, mfu 6.65%\n",
      "iter 380: loss 1.8572, time 271.53ms, mfu 6.68%\n",
      "iter 390: loss 1.8059, time 272.50ms, mfu 6.69%\n",
      "iter 400: loss 1.7858, time 273.77ms, mfu 6.71%\n",
      "iter 410: loss 1.7882, time 279.17ms, mfu 6.71%\n",
      "iter 420: loss 1.7851, time 272.55ms, mfu 6.72%\n",
      "iter 430: loss 1.7807, time 273.44ms, mfu 6.73%\n",
      "iter 440: loss 1.7627, time 272.90ms, mfu 6.74%\n",
      "iter 450: loss 1.7524, time 272.70ms, mfu 6.75%\n",
      "iter 460: loss 1.7466, time 271.89ms, mfu 6.77%\n",
      "iter 470: loss 1.7607, time 272.76ms, mfu 6.77%\n",
      "iter 480: loss 1.6932, time 275.27ms, mfu 6.78%\n",
      "iter 490: loss 1.6969, time 274.67ms, mfu 6.78%\n",
      "step 500: train loss 1.6101, val loss 1.7835\n",
      "saving checkpoint to out-kabyar-char\n",
      "iter 500: loss 1.7149, time 8221.06ms, mfu 6.12%\n",
      "iter 510: loss 1.6773, time 272.85ms, mfu 6.20%\n",
      "iter 520: loss 1.7150, time 273.81ms, mfu 6.26%\n",
      "iter 530: loss 1.6846, time 275.70ms, mfu 6.31%\n",
      "iter 540: loss 1.6216, time 276.03ms, mfu 6.36%\n",
      "iter 550: loss 1.6480, time 273.09ms, mfu 6.40%\n",
      "iter 560: loss 1.6348, time 272.15ms, mfu 6.45%\n",
      "iter 570: loss 1.6573, time 272.71ms, mfu 6.49%\n",
      "iter 580: loss 1.6519, time 272.07ms, mfu 6.53%\n",
      "iter 590: loss 1.6310, time 272.74ms, mfu 6.56%\n",
      "iter 600: loss 1.6014, time 276.04ms, mfu 6.58%\n",
      "iter 610: loss 1.5970, time 273.34ms, mfu 6.61%\n",
      "iter 620: loss 1.5798, time 271.79ms, mfu 6.63%\n",
      "iter 630: loss 1.5895, time 272.45ms, mfu 6.66%\n",
      "iter 640: loss 1.5500, time 272.42ms, mfu 6.68%\n",
      "iter 650: loss 1.5447, time 275.87ms, mfu 6.69%\n",
      "iter 660: loss 1.5481, time 275.44ms, mfu 6.70%\n",
      "iter 670: loss 1.5559, time 273.59ms, mfu 6.71%\n",
      "iter 680: loss 1.5182, time 271.88ms, mfu 6.73%\n",
      "iter 690: loss 1.5265, time 272.40ms, mfu 6.74%\n",
      "iter 700: loss 1.5264, time 272.89ms, mfu 6.75%\n",
      "iter 710: loss 1.4594, time 273.82ms, mfu 6.76%\n",
      "iter 720: loss 1.4934, time 278.63ms, mfu 6.75%\n",
      "iter 730: loss 1.5283, time 274.50ms, mfu 6.76%\n",
      "iter 740: loss 1.5544, time 271.97ms, mfu 6.77%\n",
      "step 750: train loss 1.3394, val loss 1.6776\n",
      "saving checkpoint to out-kabyar-char\n",
      "iter 750: loss 1.4988, time 8139.43ms, mfu 6.11%\n",
      "iter 760: loss 1.4327, time 273.06ms, mfu 6.19%\n",
      "iter 770: loss 1.4693, time 277.98ms, mfu 6.24%\n",
      "iter 780: loss 1.4271, time 273.19ms, mfu 6.30%\n",
      "iter 790: loss 1.4466, time 272.05ms, mfu 6.36%\n",
      "iter 800: loss 1.4490, time 272.07ms, mfu 6.41%\n",
      "iter 810: loss 1.4013, time 273.49ms, mfu 6.45%\n",
      "iter 820: loss 1.3969, time 278.26ms, mfu 6.48%\n",
      "iter 830: loss 1.4021, time 272.69ms, mfu 6.51%\n",
      "iter 840: loss 1.4177, time 271.99ms, mfu 6.55%\n",
      "iter 850: loss 1.4284, time 271.87ms, mfu 6.58%\n",
      "iter 860: loss 1.3480, time 271.81ms, mfu 6.61%\n",
      "iter 870: loss 1.4078, time 273.60ms, mfu 6.63%\n",
      "iter 880: loss 1.3921, time 272.86ms, mfu 6.65%\n",
      "iter 890: loss 1.3240, time 271.92ms, mfu 6.68%\n",
      "iter 900: loss 1.3682, time 272.51ms, mfu 6.69%\n",
      "iter 910: loss 1.3631, time 273.02ms, mfu 6.71%\n",
      "iter 920: loss 1.3442, time 274.08ms, mfu 6.72%\n",
      "iter 930: loss 1.3176, time 274.44ms, mfu 6.73%\n",
      "iter 940: loss 1.3079, time 271.65ms, mfu 6.74%\n",
      "iter 950: loss 1.2899, time 272.75ms, mfu 6.75%\n",
      "iter 960: loss 1.3157, time 272.20ms, mfu 6.77%\n",
      "iter 970: loss 1.3167, time 274.10ms, mfu 6.77%\n",
      "iter 980: loss 1.3024, time 278.04ms, mfu 6.77%\n",
      "iter 990: loss 1.3015, time 276.12ms, mfu 6.77%\n",
      "step 1000: train loss 1.1000, val loss 1.7008\n",
      "iter 1000: loss 1.2640, time 7068.10ms, mfu 6.12%\n",
      "iter 1010: loss 1.3074, time 273.25ms, mfu 6.19%\n",
      "iter 1020: loss 1.2658, time 274.82ms, mfu 6.25%\n",
      "iter 1030: loss 1.2361, time 273.00ms, mfu 6.31%\n",
      "iter 1040: loss 1.2673, time 271.91ms, mfu 6.36%\n",
      "iter 1050: loss 1.2006, time 272.61ms, mfu 6.41%\n",
      "iter 1060: loss 1.2686, time 274.01ms, mfu 6.45%\n",
      "iter 1070: loss 1.2617, time 277.82ms, mfu 6.48%\n",
      "iter 1080: loss 1.2287, time 273.54ms, mfu 6.52%\n",
      "iter 1090: loss 1.1938, time 271.67ms, mfu 6.55%\n",
      "iter 1100: loss 1.2487, time 272.69ms, mfu 6.58%\n",
      "iter 1110: loss 1.2206, time 272.66ms, mfu 6.61%\n",
      "iter 1120: loss 1.2083, time 274.31ms, mfu 6.63%\n",
      "iter 1130: loss 1.1873, time 277.67ms, mfu 6.64%\n",
      "iter 1140: loss 1.1940, time 273.08ms, mfu 6.66%\n",
      "iter 1150: loss 1.2364, time 271.49ms, mfu 6.68%\n",
      "iter 1160: loss 1.1896, time 272.61ms, mfu 6.70%\n",
      "iter 1170: loss 1.1479, time 273.57ms, mfu 6.71%\n",
      "iter 1180: loss 1.1787, time 273.76ms, mfu 6.72%\n",
      "iter 1190: loss 1.1311, time 274.62ms, mfu 6.73%\n",
      "iter 1200: loss 1.1381, time 273.24ms, mfu 6.74%\n",
      "iter 1210: loss 1.1560, time 272.24ms, mfu 6.75%\n",
      "iter 1220: loss 1.1716, time 272.26ms, mfu 6.76%\n",
      "iter 1230: loss 1.1222, time 273.39ms, mfu 6.77%\n",
      "iter 1240: loss 1.1204, time 273.08ms, mfu 6.78%\n",
      "step 1250: train loss 0.8822, val loss 1.7879\n",
      "iter 1250: loss 1.1341, time 7063.14ms, mfu 6.13%\n",
      "iter 1260: loss 1.1427, time 273.08ms, mfu 6.20%\n",
      "iter 1270: loss 1.1147, time 273.39ms, mfu 6.26%\n",
      "iter 1280: loss 1.0999, time 271.86ms, mfu 6.32%\n",
      "iter 1290: loss 1.1044, time 272.17ms, mfu 6.38%\n",
      "iter 1300: loss 1.1311, time 274.52ms, mfu 6.42%\n",
      "iter 1310: loss 1.0796, time 275.98ms, mfu 6.46%\n",
      "iter 1320: loss 1.1052, time 276.60ms, mfu 6.49%\n",
      "iter 1330: loss 1.0766, time 274.25ms, mfu 6.52%\n",
      "iter 1340: loss 1.0865, time 271.63ms, mfu 6.55%\n",
      "iter 1350: loss 1.0311, time 272.15ms, mfu 6.58%\n",
      "iter 1360: loss 1.0546, time 274.13ms, mfu 6.61%\n",
      "iter 1370: loss 1.1144, time 272.84ms, mfu 6.63%\n",
      "iter 1380: loss 1.0431, time 273.47ms, mfu 6.65%\n",
      "iter 1390: loss 1.0436, time 272.81ms, mfu 6.67%\n",
      "iter 1400: loss 1.0019, time 272.51ms, mfu 6.69%\n",
      "iter 1410: loss 1.0638, time 273.37ms, mfu 6.70%\n",
      "iter 1420: loss 0.9902, time 274.18ms, mfu 6.72%\n",
      "iter 1430: loss 1.0361, time 273.89ms, mfu 6.73%\n",
      "iter 1440: loss 1.0460, time 272.46ms, mfu 6.74%\n",
      "iter 1450: loss 1.0051, time 272.31ms, mfu 6.75%\n",
      "iter 1460: loss 1.0064, time 272.15ms, mfu 6.76%\n",
      "iter 1470: loss 1.0390, time 275.93ms, mfu 6.76%\n",
      "iter 1480: loss 1.0196, time 276.16ms, mfu 6.76%\n",
      "iter 1490: loss 0.9666, time 272.96ms, mfu 6.77%\n",
      "step 1500: train loss 0.7048, val loss 1.9023\n",
      "iter 1500: loss 1.0223, time 7077.15ms, mfu 6.12%\n",
      "iter 1510: loss 1.0027, time 277.19ms, mfu 6.18%\n",
      "iter 1520: loss 0.9823, time 272.92ms, mfu 6.25%\n",
      "iter 1530: loss 0.9777, time 272.21ms, mfu 6.31%\n",
      "iter 1540: loss 0.9852, time 272.60ms, mfu 6.36%\n",
      "iter 1550: loss 1.0092, time 274.74ms, mfu 6.41%\n",
      "iter 1560: loss 0.9782, time 273.05ms, mfu 6.45%\n",
      "iter 1570: loss 0.9524, time 271.97ms, mfu 6.49%\n",
      "iter 1580: loss 0.9440, time 271.55ms, mfu 6.53%\n",
      "iter 1590: loss 0.9252, time 273.82ms, mfu 6.56%\n",
      "iter 1600: loss 0.9703, time 273.44ms, mfu 6.59%\n",
      "iter 1610: loss 0.9465, time 275.39ms, mfu 6.61%\n",
      "iter 1620: loss 0.9402, time 274.15ms, mfu 6.63%\n",
      "iter 1630: loss 0.9469, time 272.35ms, mfu 6.65%\n",
      "iter 1640: loss 0.8946, time 271.85ms, mfu 6.67%\n",
      "iter 1650: loss 0.9571, time 272.47ms, mfu 6.69%\n",
      "iter 1660: loss 0.8947, time 275.22ms, mfu 6.70%\n",
      "iter 1670: loss 0.9310, time 274.26ms, mfu 6.71%\n",
      "iter 1680: loss 0.9241, time 272.85ms, mfu 6.73%\n",
      "iter 1690: loss 0.8893, time 272.34ms, mfu 6.74%\n",
      "iter 1700: loss 0.9188, time 272.15ms, mfu 6.75%\n",
      "iter 1710: loss 0.9309, time 274.29ms, mfu 6.76%\n",
      "iter 1720: loss 0.9360, time 276.51ms, mfu 6.76%\n",
      "iter 1730: loss 0.9329, time 276.73ms, mfu 6.76%\n",
      "iter 1740: loss 0.9295, time 274.95ms, mfu 6.76%\n",
      "step 1750: train loss 0.5680, val loss 2.0183\n",
      "iter 1750: loss 0.8916, time 7069.51ms, mfu 6.11%\n",
      "iter 1760: loss 0.8676, time 275.86ms, mfu 6.18%\n",
      "iter 1770: loss 0.8681, time 274.60ms, mfu 6.24%\n",
      "iter 1780: loss 0.8708, time 272.87ms, mfu 6.30%\n",
      "iter 1790: loss 0.8498, time 272.01ms, mfu 6.36%\n",
      "iter 1800: loss 0.8564, time 273.57ms, mfu 6.40%\n",
      "iter 1810: loss 0.8461, time 272.72ms, mfu 6.45%\n",
      "iter 1820: loss 0.8805, time 276.21ms, mfu 6.48%\n",
      "iter 1830: loss 0.8780, time 273.15ms, mfu 6.52%\n",
      "iter 1840: loss 0.8425, time 271.95ms, mfu 6.55%\n",
      "iter 1850: loss 0.8683, time 272.38ms, mfu 6.58%\n",
      "iter 1860: loss 0.8357, time 273.01ms, mfu 6.61%\n",
      "iter 1870: loss 0.8463, time 277.00ms, mfu 6.62%\n",
      "iter 1880: loss 0.8734, time 273.77ms, mfu 6.64%\n",
      "iter 1890: loss 0.8593, time 274.08ms, mfu 6.66%\n",
      "iter 1900: loss 0.8558, time 272.36ms, mfu 6.68%\n",
      "iter 1910: loss 0.8146, time 272.10ms, mfu 6.70%\n",
      "iter 1920: loss 0.8284, time 273.53ms, mfu 6.71%\n",
      "iter 1930: loss 0.8216, time 274.68ms, mfu 6.72%\n",
      "iter 1940: loss 0.8163, time 275.57ms, mfu 6.73%\n",
      "iter 1950: loss 0.8160, time 272.12ms, mfu 6.74%\n",
      "iter 1960: loss 0.8187, time 272.13ms, mfu 6.75%\n",
      "iter 1970: loss 0.7922, time 274.10ms, mfu 6.76%\n",
      "iter 1980: loss 0.7770, time 273.89ms, mfu 6.77%\n",
      "iter 1990: loss 0.8110, time 273.57ms, mfu 6.77%\n",
      "step 2000: train loss 0.4599, val loss 2.1121\n",
      "iter 2000: loss 0.7908, time 7066.76ms, mfu 6.12%\n",
      "iter 2010: loss 0.7958, time 276.02ms, mfu 6.19%\n",
      "iter 2020: loss 0.7884, time 274.10ms, mfu 6.25%\n",
      "iter 2030: loss 0.8041, time 272.31ms, mfu 6.31%\n",
      "iter 2040: loss 0.7968, time 272.30ms, mfu 6.37%\n",
      "iter 2050: loss 0.7769, time 272.70ms, mfu 6.41%\n",
      "iter 2060: loss 0.7709, time 275.07ms, mfu 6.45%\n",
      "iter 2070: loss 0.8051, time 272.90ms, mfu 6.49%\n",
      "iter 2080: loss 0.7852, time 271.91ms, mfu 6.53%\n",
      "iter 2090: loss 0.7817, time 272.69ms, mfu 6.56%\n",
      "iter 2100: loss 0.7650, time 273.08ms, mfu 6.59%\n",
      "iter 2110: loss 0.7945, time 274.82ms, mfu 6.61%\n",
      "iter 2120: loss 0.7777, time 273.56ms, mfu 6.63%\n",
      "iter 2130: loss 0.7537, time 271.56ms, mfu 6.66%\n",
      "iter 2140: loss 0.7286, time 272.94ms, mfu 6.68%\n",
      "iter 2150: loss 0.7737, time 273.01ms, mfu 6.69%\n",
      "iter 2160: loss 0.7788, time 275.76ms, mfu 6.70%\n",
      "iter 2170: loss 0.7486, time 275.05ms, mfu 6.71%\n",
      "iter 2180: loss 0.7584, time 273.50ms, mfu 6.72%\n",
      "iter 2190: loss 0.7652, time 272.48ms, mfu 6.74%\n",
      "iter 2200: loss 0.7631, time 272.18ms, mfu 6.75%\n",
      "iter 2210: loss 0.7295, time 274.41ms, mfu 6.75%\n",
      "iter 2220: loss 0.7421, time 277.19ms, mfu 6.75%\n",
      "iter 2230: loss 0.7776, time 273.02ms, mfu 6.76%\n",
      "iter 2240: loss 0.6946, time 272.46ms, mfu 6.77%\n",
      "step 2250: train loss 0.3749, val loss 2.2267\n",
      "iter 2250: loss 0.7670, time 7096.61ms, mfu 6.12%\n",
      "iter 2260: loss 0.7501, time 279.15ms, mfu 6.18%\n",
      "iter 2270: loss 0.7039, time 273.33ms, mfu 6.24%\n",
      "iter 2280: loss 0.7146, time 272.22ms, mfu 6.31%\n",
      "iter 2290: loss 0.7129, time 273.46ms, mfu 6.36%\n",
      "iter 2300: loss 0.7385, time 273.49ms, mfu 6.41%\n",
      "iter 2310: loss 0.7213, time 273.22ms, mfu 6.45%\n",
      "iter 2320: loss 0.6931, time 274.71ms, mfu 6.48%\n",
      "iter 2330: loss 0.6834, time 273.22ms, mfu 6.52%\n",
      "iter 2340: loss 0.7028, time 271.87ms, mfu 6.55%\n",
      "iter 2350: loss 0.6927, time 272.74ms, mfu 6.58%\n",
      "iter 2360: loss 0.7087, time 274.09ms, mfu 6.61%\n",
      "iter 2370: loss 0.7031, time 273.96ms, mfu 6.63%\n",
      "iter 2380: loss 0.7090, time 273.44ms, mfu 6.65%\n",
      "iter 2390: loss 0.6962, time 271.81ms, mfu 6.67%\n",
      "iter 2400: loss 0.6909, time 272.34ms, mfu 6.69%\n",
      "iter 2410: loss 0.6970, time 273.25ms, mfu 6.71%\n",
      "iter 2420: loss 0.6923, time 275.10ms, mfu 6.71%\n",
      "iter 2430: loss 0.6982, time 274.57ms, mfu 6.72%\n",
      "iter 2440: loss 0.6792, time 277.66ms, mfu 6.72%\n",
      "iter 2450: loss 0.6743, time 281.00ms, mfu 6.72%\n",
      "iter 2460: loss 0.6871, time 280.17ms, mfu 6.71%\n",
      "iter 2470: loss 0.6869, time 280.23ms, mfu 6.71%\n",
      "iter 2480: loss 0.6705, time 279.97ms, mfu 6.70%\n",
      "iter 2490: loss 0.6383, time 280.00ms, mfu 6.70%\n",
      "step 2500: train loss 0.3156, val loss 2.3165\n",
      "iter 2500: loss 0.6715, time 7239.59ms, mfu 6.06%\n",
      "iter 2510: loss 0.6891, time 279.95ms, mfu 6.12%\n",
      "iter 2520: loss 0.6262, time 274.32ms, mfu 6.19%\n",
      "iter 2530: loss 0.6623, time 272.71ms, mfu 6.25%\n",
      "iter 2540: loss 0.6454, time 273.80ms, mfu 6.31%\n",
      "iter 2550: loss 0.6390, time 272.17ms, mfu 6.37%\n",
      "iter 2560: loss 0.6498, time 271.94ms, mfu 6.42%\n",
      "iter 2570: loss 0.6286, time 271.93ms, mfu 6.46%\n",
      "iter 2580: loss 0.6863, time 273.46ms, mfu 6.50%\n",
      "iter 2590: loss 0.6409, time 275.66ms, mfu 6.53%\n",
      "iter 2600: loss 0.6578, time 273.72ms, mfu 6.56%\n",
      "iter 2610: loss 0.6205, time 273.24ms, mfu 6.58%\n",
      "iter 2620: loss 0.6444, time 272.73ms, mfu 6.61%\n",
      "iter 2630: loss 0.6324, time 272.38ms, mfu 6.64%\n",
      "iter 2640: loss 0.6300, time 275.62ms, mfu 6.65%\n",
      "iter 2650: loss 0.6366, time 275.81ms, mfu 6.66%\n",
      "iter 2660: loss 0.6507, time 273.72ms, mfu 6.68%\n",
      "iter 2670: loss 0.6330, time 273.54ms, mfu 6.69%\n",
      "iter 2680: loss 0.6221, time 274.09ms, mfu 6.71%\n",
      "iter 2690: loss 0.6190, time 274.04ms, mfu 6.72%\n",
      "iter 2700: loss 0.6312, time 276.87ms, mfu 6.72%\n",
      "iter 2710: loss 0.6133, time 273.33ms, mfu 6.73%\n",
      "iter 2720: loss 0.6123, time 271.91ms, mfu 6.75%\n",
      "iter 2730: loss 0.6075, time 272.38ms, mfu 6.76%\n",
      "iter 2740: loss 0.6143, time 273.02ms, mfu 6.77%\n",
      "step 2750: train loss 0.2641, val loss 2.4233\n",
      "iter 2750: loss 0.6109, time 7071.96ms, mfu 6.12%\n",
      "iter 2760: loss 0.6219, time 274.38ms, mfu 6.19%\n",
      "iter 2770: loss 0.6141, time 274.07ms, mfu 6.25%\n",
      "iter 2780: loss 0.5957, time 274.64ms, mfu 6.30%\n",
      "iter 2790: loss 0.6001, time 274.65ms, mfu 6.35%\n",
      "iter 2800: loss 0.6079, time 272.50ms, mfu 6.40%\n",
      "iter 2810: loss 0.6121, time 274.31ms, mfu 6.44%\n",
      "iter 2820: loss 0.5986, time 273.32ms, mfu 6.48%\n",
      "iter 2830: loss 0.6245, time 274.73ms, mfu 6.52%\n",
      "iter 2840: loss 0.5877, time 273.54ms, mfu 6.55%\n",
      "iter 2850: loss 0.6173, time 272.00ms, mfu 6.58%\n",
      "iter 2860: loss 0.6014, time 273.36ms, mfu 6.60%\n",
      "iter 2870: loss 0.5906, time 273.27ms, mfu 6.63%\n",
      "iter 2880: loss 0.6143, time 279.45ms, mfu 6.63%\n",
      "iter 2890: loss 0.5925, time 273.62ms, mfu 6.65%\n",
      "iter 2900: loss 0.5913, time 271.73ms, mfu 6.68%\n",
      "iter 2910: loss 0.6152, time 272.24ms, mfu 6.69%\n",
      "iter 2920: loss 0.5939, time 273.65ms, mfu 6.71%\n",
      "iter 2930: loss 0.5827, time 276.66ms, mfu 6.71%\n",
      "iter 2940: loss 0.5835, time 280.72ms, mfu 6.71%\n",
      "iter 2950: loss 0.5762, time 276.19ms, mfu 6.71%\n",
      "iter 2960: loss 0.5761, time 273.03ms, mfu 6.73%\n",
      "iter 2970: loss 0.5766, time 273.43ms, mfu 6.74%\n",
      "iter 2980: loss 0.5693, time 277.45ms, mfu 6.74%\n",
      "iter 2990: loss 0.5680, time 275.20ms, mfu 6.74%\n",
      "step 3000: train loss 0.2289, val loss 2.4856\n",
      "iter 3000: loss 0.5916, time 7062.63ms, mfu 6.09%\n",
      "iter 3010: loss 0.5859, time 275.28ms, mfu 6.16%\n",
      "iter 3020: loss 0.5768, time 272.87ms, mfu 6.23%\n",
      "iter 3030: loss 0.5665, time 272.01ms, mfu 6.29%\n",
      "iter 3040: loss 0.5563, time 272.68ms, mfu 6.35%\n",
      "iter 3050: loss 0.5551, time 273.73ms, mfu 6.40%\n",
      "iter 3060: loss 0.5712, time 278.32ms, mfu 6.43%\n",
      "iter 3070: loss 0.5795, time 273.99ms, mfu 6.47%\n",
      "iter 3080: loss 0.5949, time 274.11ms, mfu 6.50%\n",
      "iter 3090: loss 0.5651, time 273.25ms, mfu 6.54%\n",
      "iter 3100: loss 0.5905, time 272.54ms, mfu 6.57%\n",
      "iter 3110: loss 0.5622, time 273.81ms, mfu 6.59%\n",
      "iter 3120: loss 0.5695, time 275.68ms, mfu 6.61%\n",
      "iter 3130: loss 0.5571, time 274.07ms, mfu 6.63%\n",
      "iter 3140: loss 0.5637, time 272.21ms, mfu 6.66%\n",
      "iter 3150: loss 0.5772, time 273.22ms, mfu 6.67%\n",
      "iter 3160: loss 0.5514, time 274.71ms, mfu 6.69%\n",
      "iter 3170: loss 0.5332, time 275.57ms, mfu 6.70%\n",
      "iter 3180: loss 0.5703, time 273.96ms, mfu 6.71%\n",
      "iter 3190: loss 0.5461, time 277.53ms, mfu 6.71%\n",
      "iter 3200: loss 0.5372, time 280.45ms, mfu 6.71%\n",
      "iter 3210: loss 0.5299, time 279.86ms, mfu 6.70%\n",
      "iter 3220: loss 0.5403, time 280.60ms, mfu 6.70%\n",
      "iter 3230: loss 0.5411, time 280.83ms, mfu 6.69%\n",
      "iter 3240: loss 0.5429, time 279.93ms, mfu 6.69%\n",
      "step 3250: train loss 0.1990, val loss 2.5720\n",
      "iter 3250: loss 0.5336, time 7247.94ms, mfu 6.05%\n",
      "iter 3260: loss 0.5168, time 280.69ms, mfu 6.11%\n",
      "iter 3270: loss 0.5499, time 279.83ms, mfu 6.17%\n",
      "iter 3280: loss 0.5179, time 279.94ms, mfu 6.22%\n",
      "iter 3290: loss 0.5430, time 285.27ms, mfu 6.25%\n",
      "iter 3300: loss 0.5251, time 280.25ms, mfu 6.29%\n",
      "iter 3310: loss 0.5263, time 272.80ms, mfu 6.35%\n",
      "iter 3320: loss 0.5140, time 272.95ms, mfu 6.40%\n",
      "iter 3330: loss 0.5586, time 278.03ms, mfu 6.43%\n",
      "iter 3340: loss 0.5380, time 275.23ms, mfu 6.47%\n",
      "iter 3350: loss 0.5475, time 272.90ms, mfu 6.50%\n",
      "iter 3360: loss 0.5138, time 272.10ms, mfu 6.54%\n",
      "iter 3370: loss 0.5218, time 272.58ms, mfu 6.57%\n",
      "iter 3380: loss 0.5280, time 273.27ms, mfu 6.60%\n",
      "iter 3390: loss 0.5020, time 275.97ms, mfu 6.61%\n",
      "iter 3400: loss 0.5363, time 273.01ms, mfu 6.64%\n",
      "iter 3410: loss 0.5083, time 272.06ms, mfu 6.66%\n",
      "iter 3420: loss 0.5019, time 271.80ms, mfu 6.68%\n",
      "iter 3430: loss 0.5103, time 273.10ms, mfu 6.70%\n",
      "iter 3440: loss 0.4863, time 276.78ms, mfu 6.70%\n",
      "iter 3450: loss 0.4907, time 273.29ms, mfu 6.72%\n",
      "iter 3460: loss 0.4882, time 272.42ms, mfu 6.73%\n",
      "iter 3470: loss 0.5164, time 272.53ms, mfu 6.74%\n",
      "iter 3480: loss 0.5129, time 275.00ms, mfu 6.75%\n",
      "iter 3490: loss 0.5028, time 276.26ms, mfu 6.75%\n",
      "step 3500: train loss 0.1776, val loss 2.6399\n",
      "iter 3500: loss 0.5207, time 7086.74ms, mfu 6.10%\n",
      "iter 3510: loss 0.5316, time 272.60ms, mfu 6.18%\n",
      "iter 3520: loss 0.4855, time 273.97ms, mfu 6.24%\n",
      "iter 3530: loss 0.5064, time 273.28ms, mfu 6.30%\n",
      "iter 3540: loss 0.4991, time 272.00ms, mfu 6.36%\n",
      "iter 3550: loss 0.5084, time 272.46ms, mfu 6.41%\n",
      "iter 3560: loss 0.5064, time 272.36ms, mfu 6.45%\n",
      "iter 3570: loss 0.4999, time 273.52ms, mfu 6.49%\n",
      "iter 3580: loss 0.4947, time 272.81ms, mfu 6.53%\n",
      "iter 3590: loss 0.4922, time 272.50ms, mfu 6.56%\n",
      "iter 3600: loss 0.5054, time 272.56ms, mfu 6.59%\n",
      "iter 3610: loss 0.4843, time 275.12ms, mfu 6.61%\n",
      "iter 3620: loss 0.4805, time 274.17ms, mfu 6.63%\n",
      "iter 3630: loss 0.4748, time 276.31ms, mfu 6.64%\n",
      "iter 3640: loss 0.5198, time 273.28ms, mfu 6.66%\n",
      "iter 3650: loss 0.4880, time 271.77ms, mfu 6.68%\n",
      "iter 3660: loss 0.4887, time 273.21ms, mfu 6.70%\n",
      "iter 3670: loss 0.4595, time 273.26ms, mfu 6.71%\n",
      "iter 3680: loss 0.4997, time 275.10ms, mfu 6.72%\n",
      "iter 3690: loss 0.4973, time 274.92ms, mfu 6.73%\n",
      "iter 3700: loss 0.4870, time 272.02ms, mfu 6.74%\n",
      "iter 3710: loss 0.4801, time 272.54ms, mfu 6.75%\n",
      "iter 3720: loss 0.4734, time 272.62ms, mfu 6.76%\n",
      "iter 3730: loss 0.5030, time 279.31ms, mfu 6.76%\n",
      "iter 3740: loss 0.4997, time 274.70ms, mfu 6.76%\n",
      "step 3750: train loss 0.1623, val loss 2.7005\n",
      "iter 3750: loss 0.4638, time 7065.99ms, mfu 6.11%\n",
      "iter 3760: loss 0.4871, time 273.63ms, mfu 6.18%\n",
      "iter 3770: loss 0.4656, time 273.79ms, mfu 6.25%\n",
      "iter 3780: loss 0.4852, time 272.09ms, mfu 6.31%\n",
      "iter 3790: loss 0.4828, time 272.22ms, mfu 6.36%\n",
      "iter 3800: loss 0.4794, time 273.27ms, mfu 6.41%\n",
      "iter 3810: loss 0.4980, time 273.42ms, mfu 6.45%\n",
      "iter 3820: loss 0.4767, time 273.35ms, mfu 6.49%\n",
      "iter 3830: loss 0.4567, time 272.28ms, mfu 6.53%\n",
      "iter 3840: loss 0.4789, time 272.06ms, mfu 6.56%\n",
      "iter 3850: loss 0.4681, time 275.71ms, mfu 6.58%\n",
      "iter 3860: loss 0.4767, time 273.18ms, mfu 6.61%\n",
      "iter 3870: loss 0.4502, time 271.91ms, mfu 6.64%\n",
      "iter 3880: loss 0.4867, time 272.68ms, mfu 6.66%\n",
      "iter 3890: loss 0.4660, time 273.01ms, mfu 6.68%\n",
      "iter 3900: loss 0.4519, time 273.51ms, mfu 6.69%\n",
      "iter 3910: loss 0.4727, time 273.21ms, mfu 6.71%\n",
      "iter 3920: loss 0.4862, time 271.89ms, mfu 6.72%\n",
      "iter 3930: loss 0.4615, time 272.37ms, mfu 6.74%\n",
      "iter 3940: loss 0.4680, time 273.42ms, mfu 6.75%\n",
      "iter 3950: loss 0.4646, time 274.00ms, mfu 6.75%\n",
      "iter 3960: loss 0.4719, time 279.01ms, mfu 6.75%\n",
      "iter 3970: loss 0.4766, time 275.93ms, mfu 6.75%\n",
      "iter 3980: loss 0.4536, time 274.25ms, mfu 6.76%\n",
      "iter 3990: loss 0.4634, time 271.89ms, mfu 6.77%\n",
      "step 4000: train loss 0.1494, val loss 2.7453\n",
      "iter 4000: loss 0.4599, time 7117.95ms, mfu 6.12%\n",
      "iter 4010: loss 0.4639, time 276.04ms, mfu 6.18%\n",
      "iter 4020: loss 0.4575, time 273.53ms, mfu 6.25%\n",
      "iter 4030: loss 0.4771, time 272.84ms, mfu 6.31%\n",
      "iter 4040: loss 0.4639, time 273.10ms, mfu 6.36%\n",
      "iter 4050: loss 0.4547, time 276.72ms, mfu 6.40%\n",
      "iter 4060: loss 0.4317, time 277.82ms, mfu 6.43%\n",
      "iter 4070: loss 0.4394, time 272.45ms, mfu 6.47%\n",
      "iter 4080: loss 0.4623, time 272.86ms, mfu 6.51%\n",
      "iter 4090: loss 0.4397, time 272.42ms, mfu 6.55%\n",
      "iter 4100: loss 0.4641, time 272.67ms, mfu 6.58%\n",
      "iter 4110: loss 0.4378, time 279.09ms, mfu 6.59%\n",
      "iter 4120: loss 0.4537, time 273.60ms, mfu 6.61%\n",
      "iter 4130: loss 0.4555, time 273.18ms, mfu 6.64%\n",
      "iter 4140: loss 0.4563, time 272.36ms, mfu 6.66%\n",
      "iter 4150: loss 0.4361, time 278.38ms, mfu 6.66%\n",
      "iter 4160: loss 0.4532, time 274.46ms, mfu 6.68%\n",
      "iter 4170: loss 0.4387, time 272.71ms, mfu 6.69%\n",
      "iter 4180: loss 0.4432, time 272.33ms, mfu 6.71%\n",
      "iter 4190: loss 0.4429, time 272.27ms, mfu 6.73%\n",
      "iter 4200: loss 0.4406, time 272.94ms, mfu 6.74%\n",
      "iter 4210: loss 0.4521, time 278.16ms, mfu 6.74%\n",
      "iter 4220: loss 0.4445, time 281.01ms, mfu 6.73%\n",
      "iter 4230: loss 0.4341, time 275.95ms, mfu 6.73%\n",
      "iter 4240: loss 0.4439, time 271.94ms, mfu 6.75%\n",
      "step 4250: train loss 0.1411, val loss 2.8063\n",
      "iter 4250: loss 0.4313, time 7103.89ms, mfu 6.10%\n",
      "iter 4260: loss 0.4466, time 274.20ms, mfu 6.17%\n",
      "iter 4270: loss 0.4500, time 272.30ms, mfu 6.24%\n",
      "iter 4280: loss 0.4558, time 272.09ms, mfu 6.30%\n",
      "iter 4290: loss 0.4536, time 273.69ms, mfu 6.35%\n",
      "iter 4300: loss 0.4663, time 278.24ms, mfu 6.39%\n",
      "iter 4310: loss 0.4455, time 273.03ms, mfu 6.44%\n",
      "iter 4320: loss 0.4515, time 271.74ms, mfu 6.48%\n",
      "iter 4330: loss 0.4402, time 272.20ms, mfu 6.52%\n",
      "iter 4340: loss 0.4338, time 274.02ms, mfu 6.55%\n",
      "iter 4350: loss 0.4450, time 275.31ms, mfu 6.57%\n",
      "iter 4360: loss 0.4258, time 275.13ms, mfu 6.59%\n",
      "iter 4370: loss 0.4420, time 272.53ms, mfu 6.62%\n",
      "iter 4380: loss 0.4502, time 272.60ms, mfu 6.64%\n",
      "iter 4390: loss 0.4381, time 273.39ms, mfu 6.66%\n",
      "iter 4400: loss 0.4478, time 274.98ms, mfu 6.68%\n",
      "iter 4410: loss 0.4262, time 272.98ms, mfu 6.69%\n",
      "iter 4420: loss 0.4248, time 274.05ms, mfu 6.70%\n",
      "iter 4430: loss 0.4291, time 272.35ms, mfu 6.72%\n",
      "iter 4440: loss 0.4269, time 272.66ms, mfu 6.73%\n",
      "iter 4450: loss 0.4275, time 272.88ms, mfu 6.74%\n",
      "iter 4460: loss 0.4295, time 273.55ms, mfu 6.75%\n",
      "iter 4470: loss 0.4239, time 272.95ms, mfu 6.76%\n",
      "iter 4480: loss 0.4249, time 272.48ms, mfu 6.77%\n",
      "iter 4490: loss 0.4318, time 273.59ms, mfu 6.78%\n",
      "step 4500: train loss 0.1341, val loss 2.8384\n",
      "iter 4500: loss 0.4432, time 7129.08ms, mfu 6.13%\n",
      "iter 4510: loss 0.4354, time 273.79ms, mfu 6.20%\n",
      "iter 4520: loss 0.4531, time 272.87ms, mfu 6.26%\n",
      "iter 4530: loss 0.4319, time 273.40ms, mfu 6.32%\n",
      "iter 4540: loss 0.4239, time 273.82ms, mfu 6.37%\n",
      "iter 4550: loss 0.3983, time 273.93ms, mfu 6.41%\n",
      "iter 4560: loss 0.4402, time 272.18ms, mfu 6.46%\n",
      "iter 4570: loss 0.4212, time 272.87ms, mfu 6.50%\n",
      "iter 4580: loss 0.4081, time 274.47ms, mfu 6.53%\n",
      "iter 4590: loss 0.4074, time 274.41ms, mfu 6.56%\n",
      "iter 4600: loss 0.4268, time 275.39ms, mfu 6.58%\n",
      "iter 4610: loss 0.4320, time 272.76ms, mfu 6.61%\n",
      "iter 4620: loss 0.4198, time 272.53ms, mfu 6.63%\n",
      "iter 4630: loss 0.4227, time 272.74ms, mfu 6.65%\n",
      "iter 4640: loss 0.4205, time 272.69ms, mfu 6.67%\n",
      "iter 4650: loss 0.4207, time 273.48ms, mfu 6.69%\n",
      "iter 4660: loss 0.4232, time 273.39ms, mfu 6.70%\n",
      "iter 4670: loss 0.4083, time 272.33ms, mfu 6.72%\n",
      "iter 4680: loss 0.4268, time 271.97ms, mfu 6.73%\n",
      "iter 4690: loss 0.4226, time 274.88ms, mfu 6.74%\n",
      "iter 4700: loss 0.4235, time 274.75ms, mfu 6.75%\n",
      "iter 4710: loss 0.4237, time 273.67ms, mfu 6.75%\n",
      "iter 4720: loss 0.4190, time 272.07ms, mfu 6.77%\n",
      "iter 4730: loss 0.4096, time 271.75ms, mfu 6.78%\n",
      "iter 4740: loss 0.4083, time 272.51ms, mfu 6.78%\n",
      "step 4750: train loss 0.1298, val loss 2.8631\n",
      "iter 4750: loss 0.4045, time 7089.89ms, mfu 6.13%\n",
      "iter 4760: loss 0.4094, time 272.21ms, mfu 6.21%\n",
      "iter 4770: loss 0.4124, time 273.87ms, mfu 6.27%\n",
      "iter 4780: loss 0.4013, time 278.13ms, mfu 6.31%\n",
      "iter 4790: loss 0.4253, time 274.25ms, mfu 6.36%\n",
      "iter 4800: loss 0.4282, time 272.99ms, mfu 6.41%\n",
      "iter 4810: loss 0.4248, time 272.59ms, mfu 6.45%\n",
      "iter 4820: loss 0.4337, time 273.45ms, mfu 6.49%\n",
      "iter 4830: loss 0.4181, time 272.61ms, mfu 6.53%\n",
      "iter 4840: loss 0.4220, time 276.76ms, mfu 6.55%\n",
      "iter 4850: loss 0.4099, time 273.95ms, mfu 6.58%\n",
      "iter 4860: loss 0.4228, time 271.98ms, mfu 6.61%\n",
      "iter 4870: loss 0.4238, time 272.63ms, mfu 6.63%\n",
      "iter 4880: loss 0.3961, time 276.20ms, mfu 6.64%\n",
      "iter 4890: loss 0.4323, time 278.25ms, mfu 6.65%\n",
      "iter 4900: loss 0.3970, time 273.53ms, mfu 6.67%\n",
      "iter 4910: loss 0.4145, time 272.65ms, mfu 6.69%\n",
      "iter 4920: loss 0.4079, time 272.26ms, mfu 6.71%\n",
      "iter 4930: loss 0.4045, time 273.66ms, mfu 6.72%\n",
      "iter 4940: loss 0.4087, time 275.30ms, mfu 6.72%\n",
      "iter 4950: loss 0.4027, time 274.59ms, mfu 6.73%\n",
      "iter 4960: loss 0.4008, time 272.13ms, mfu 6.75%\n",
      "iter 4970: loss 0.4166, time 273.17ms, mfu 6.76%\n",
      "iter 4980: loss 0.3987, time 272.84ms, mfu 6.76%\n",
      "iter 4990: loss 0.4194, time 273.11ms, mfu 6.77%\n",
      "step 5000: train loss 0.1268, val loss 2.8879\n",
      "iter 5000: loss 0.3983, time 7120.80ms, mfu 6.12%\n",
      "\n",
      "real\t25m59.938s\n",
      "user\t25m39.636s\n",
      "sys\t0m13.360s\n"
     ]
    }
   ],
   "source": [
    "!time python -m torch.distributed.launch --use-env train.py ./config/train_kabyar_char.py | tee train-kabyar-char.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 124M\n",
      "-rw-rw-r-- 1 rnd rnd 124M Apr 18 03:28 ckpt.pt\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ./out-kabyar-char/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   423186   2410706 129382378 ./out-kabyar-char/ckpt.pt\n"
     ]
    }
   ],
   "source": [
    "!wc ./out-kabyar-char/ckpt.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing-1 (char Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rnd/anaconda3/envs/nanoGPT/lib/python3.8/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use-env is set by default in torchrun.\n",
      "If your script expects `--local-rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  warnings.warn(\n",
      "Overriding: out_dir = out-kabyar-char\n",
      "number of parameters: 10.68M\n",
      "Loading meta from data/kabyar_char/meta.pkl...\n",
      "\n",
      "ကိုယ့်လက်ချက်တွေက လက်ရှိုက်နှိပ်တယ်\n",
      "ကျားမြန်မာတွေက လက်စေးနေတယ်။\n",
      "\n",
      "Title: ကျားမျှော်လင့်ချက်\n",
      "By: နီတိ\n",
      "ကော်ပျော်ရွှင်လန်းနေတဲ့\n",
      "တိတ်တတ်တယ်\n",
      "ကျားမျှော်လင့်ခြင်းတွေက\n",
      "ပျော်ရွှင်တွေကို\n",
      "ကျားသန့်နေခဲ့။\n",
      "ကောင်းကင်ဟာ\n",
      "သူ့နှလုံးသားတွေနဲ့\n",
      "ကျားကျားခဲ့တယ်\n",
      "ကျားခဲ့ရတဲ့ လက်မျှားကျားတွေထဲ\n",
      "ကျားကျားထည့်ရတဲ့ စကားလုံးထဲ\n",
      "သူဟာ\n",
      "အားလုံးတွေနဲ့ လက်မတတ်တစ်ပုဒ်ကို\n",
      "မျက်ရည်ဖွယ်ရောက်လုံး\n",
      "ကျားလည်ပြုံးနေရတဲ့ အိပ်ရာဝင်/ပျံသွားတာကို\n",
      "ကျားလည်ပြုံးနေပြီ\n",
      "ကျောင်းလာနေတဲ့ စကားလုံးကို\n",
      "မျက်ရည်ကျရည်တွေလာနေပြီး\n",
      "အခု ကျုပ်တို့ဟာ\n",
      "အခု ရောက်ရှိတဲ့ ကလေး\n",
      "---------------\n",
      "\n",
      "ရွှေပြည် စာနိုင်ငံတွင်\n",
      "တည့်မြင်ရ အားထုတ်ပွဲ၍\n",
      "ဝန်းကျင်ရခံ ဧရာဝတီမှ\n",
      "ပြည်ရွယ်ဝင်း ပျို့ရွှင်ချ်\n",
      "လှည်းပြည်ရွှင်ကြည် ဝယ်ကွင်းပြီ ။\n",
      "\n",
      "Title: ဝန်းကျင်နေလေရာ\n",
      "By: ပခုက္ကူဦးအုံးဖေ\n",
      "ပြည်ရွယ်သွင်းဝေ၊ အခြေဖြာဝေနှင့်\n",
      "ဘဝင်းကြွားဝေကြည်၊ ဂုဏ်ပြည်သူ့ရွာသည်\n",
      "ယာစခန်းရွှမ်း၊ ဝန်းကျင်လွန်းစည်ကြည်။\n",
      "အပြန်းချုပ်ရွှင်၊ ပန်းလန်းဝေဖြာဝေ\n",
      "မြို့နိုင်ငံအလှ၊ ပွင့်ဝေသီ\n",
      "တောဗျိမ့်ငြိမ့်ငြိမ့်၊ သိမ့်သိမ့်ငြိမ့်ငြိမ့်\n",
      "ပျို့ရွှင်လန်းဖြိုး၊ ကြီးမြင့်မားတို့\n",
      "အဖြိုးများပြား၊ စေတနာသနားနှင့်\n",
      "ပြည်ရွာထွန်းပေးဝေး၊ ပေးလွှေးလွင်းသည်\n",
      "ဝေးလွင့်လွင်\n",
      "---------------\n",
      "\n",
      "သိလားရာမှ မရှိဘူး\n",
      "သိပ္ပာယ် ပရိဘတ်ထဲ သိတာ သိပ်ပိုပင် ထည်ဝါးတယ်\n",
      "ဒါပေမယ့် သိပ်က မရှိဘူးလား\n",
      "ဒီငယ်ချင်း ငါ့ကို သိရှိတဲ့ အိမ်ကိုယ်\n",
      "ကျန်မှာ ကျန်မစားတတ်တယ်\n",
      "အမိန့်ကို မကျန်ခဲ့ဘူး\n",
      "ခုတော့ အဆုံးအစားတွေ ရှိခဲ့သလို\n",
      "ကျားရေးသားတွေ နားထမ်းလာတယ်\n",
      "ဒီမြေက အရေးမမှာ အမှတ်တရ\n",
      "လူတစ်ယောက်က လူတစ်ယောက်က လူတစ်ယောက်ရဲ့ ဟိုက်ကို ဖြစ်ချင်တယ်\n",
      "‘လူတွေ မှတ်တော့ရတဲ့ ကျွန်မသိတဲ့ မိသားစုအုပ်ထဲ\n",
      "လူတွေ ဘာကြားလိုက်ဖို့ စစ်သားတော့\n",
      "မှတ်ကြားထားတဲ့ ဓားလာတဲ့ ကျွန်မ ငြိမ်းချမ်းမှုတွေ\n",
      "စစ်ထားပစ်လိုက်ရင်\n",
      "အဖုတ်အချစ်နဲ့ ဖြစ်နေခဲ့တဲ့\n",
      "ဒါကြောင့် ပန်းတွ\n",
      "---------------\n",
      "\n",
      "အတွေးဆုံး မိုးထုံးစည်း\n",
      "မိုးသို့ စည်းရုံး\n",
      "မွှေးမင်းရိုးနှင့် စည်းရိုးရှည်\n",
      "တောင်ပံ့ ကြိုးပမ်း ဆွေမတ်။ ။\n",
      "\n",
      "Title: အမိုက်အတွေးအပ်\n",
      "By: ပခုက္ကူဦးအုံးဖေ\n",
      "မိုက်တွင် အားသစ်ဆောင်\n",
      "အသွေးအချစ်\n",
      "လွမ်းစစ်ပါး၊ သူတို့မောင်\n",
      "ချစ်ရိုင်းဆော့ချစ်ရင်း။\n",
      "မျှော်လင့်ချက်သိ\n",
      "လင့်ချက်ခဲ့သည်မှာ\n",
      "အမှားမဲ့သူ။\n",
      "\n",
      "Title: နှစ်ပုဒ်ပင်\n",
      "By: မြင်းမူမောင်နိုင်မိုး\n",
      "လက်ထဲက စိုးရိပ်\n",
      "လက်နက်သလို\n",
      "ဖြစ်ချင်သော မျက်ရည်မှာ။\n",
      "မျက်ရည်စွဲစိပ်ဝေး မျက်နှာဖြင့်\n",
      "အသည်းအားထုတ်၊ ဂိုးတွေလုပ်ကို\n",
      "ပြုသင်းမောပျက်၊ လက်ရောင်ချောင်းနှင့်\n",
      "အဖျက်အလျော့၊ ကျော့ငှက်မှုတ်တော့\n",
      "---------------\n",
      "\n",
      "ကိုယ့်အားလုံးကို\n",
      "အားလုံးသား ရှိကြသတဲ့ ။\n",
      "\n",
      "Title: (၂၀)\n",
      "By: မြင့်စိုးလှိုင်\n",
      "နှုတ်သီးစား\n",
      "ရာသီးသီးများ\n",
      "အားတွေလက်\n",
      "နွေစိုက်များက ပြန်၍တွေ\n",
      "အားလုံးဟာ\n",
      "ကိုယ့်အိပ်ရာက အဖြေ\n",
      "လွမ်းစစ်ချစ်တဲ့ကွယ်\n",
      "မင်းအကြောင်း\n",
      "အဖြေအနှောင်း\n",
      "စစ်ချစ် ပြန်ပြန်ပြောနေဖို့\n",
      "ရင်သန်းခုန်မှာ\n",
      "လွမ်းစို့လိုက်ဖက်\n",
      "ကိုယ့်အိပ်မက် ဖြစ်လောင်ပေါ့ကွယ်\n",
      "ကမ္ဘာကြောင့်တွေ ၊ မင်းအနွေ\n",
      "ဂုဏ်ရည်စူးခြွေ၊ လေယူထွေဝေပြည်\n",
      "လေအင်းကျွေးနှံ့၊ မူဟန်ရဲရဲခဲ့ ။\n",
      "\n",
      "Title: နေ့ပုံနှင့် ကျွန်တော်တို့\n",
      "By: မောင်စိန်ဝင်း\n",
      "အလင်းပြင်းသည့်အလား\n",
      "ကြည့်လည်စမ်းမော၊ ဦးတော်ရှိသည်\n",
      "ကြည့်ပါစေလိမ\n",
      "---------------\n",
      "\n",
      "ရေအလှူ လေအလှူတွေနဲ့\n",
      "မြူးတွန့်ဆင်းမှာ ရပ်လှူရှုမရှိတဲ့\n",
      "အချိန်တကာ ဆာလှူငယ်များတွေနဲ့\n",
      "ဒေါသအား ယူလှတယ် မြင်တယ် မြင်ယူခဲ့တယ်\n",
      "ဒါပေမဲ့ ငါကိုယ် အတူတူ အမြဲတွေနဲ့\n",
      "ဖူးငုံကို ဖမ်းထားရတာတွေးတာပေါ့ အစိုးရအောင် အကြောင်း အရိုင်းအဝေးတာများလည်း\n",
      "ကျွန်တော်တို့ကို အထားအရလည်း လှပ်ရှပ်ရှပ်သော မျက်နှာထိုင်တွေက\n",
      "ဒါကို မြင်တယ်\n",
      "ဒီလိုနဲ့ ကျွန်တော့်အကွာအတိုင်း ရှိတယ်\n",
      "ဒဏ်ရာတွေကို မျက်စိစ္စမပျောက်နေတဲ့ ပြုံးတွေအခါ တစ်ကောင်ထွက်ပေါက်ဖို့\n",
      "ဒေါင်းခန်းတွေ ခြေထောက်နေဆဲမှာ တစ်ယောက်ပြန်လို့\n",
      "ဒီတစ်ယောက်က ဒေါင်းခန်းတလောက် ထပ်တစ်ခွက်က ပင်လ\n",
      "---------------\n",
      "\n",
      "ပန်းတွေပေါ်မှာ ။\n",
      "ညအသီးတံတားရှင်\n",
      "စွဲကြည်နူးရွှင်ကျ၊ ထင်ရှာထလျက်\n",
      "လိုက်ပါသည်\n",
      "မြို့ရပ်များ၊\n",
      "ထင်ရက်များစွာ၊ မြင်နာသည်\n",
      "ဉာဏ်အပ်ရွက်၊ ခရီးတွက်မြက်ကို\n",
      "ဆက်လက်ပြက်လောင်၊ နူးညွတ်အိမ်ထောင်ဝယ်\n",
      "နှောင်တွေ့ပျော်ရွက်၊ တမ်းပျက်မိ၍\n",
      "သက်မိုက်ခင်း၊ ရွှင်ရင်တွက်လျက်\n",
      "မြင်းသက်ဝေသီ၊ အသစ်မြင်လေသည်\n",
      "ဪ.....ကိုယ်စီးလျှင်၊ မြစ်ပျက်၍\n",
      "အေးစက်မြှောင်အောင်ဆန်း၊ ကြုံးဇူးယမ်းဖို့\n",
      "သစ်ပန်းလည်းမြန့်စေ၊ စေတီလွမ်းသည်\n",
      "ဓလေ့ချစ်သည်လမ်း၊ မေတ္တာဖြင့်\n",
      "ပန်းထွေသရီ၊ စိတ်ကြည်မြိုက်ခိုက်ခိုက်\n",
      "ပန်းတိုက်ဝင် လန်းလန်းကြည် ။\n",
      "အလွမ်းပြည်ပ ၊ ရေအလှဖြင့်\n",
      "တွေ့လ\n",
      "---------------\n",
      "\n",
      "မောင်ပေါ်မှာစိုး ။\n",
      "အောက်စိမ့်လန်း ၊ ချစ်စမိုး\n",
      "ဝမ်းမြန်မာလှမ်းစိုး ၊ ကြားလိုက်နိုင်\n",
      "ဝမ်းမြန်မာ စိတ်ကိုးစဉ်လို့\n",
      "အလှသာ အကြည့်ကြုံ ။\n",
      "\n",
      "Title: အသင့် လမ်းတစ်ဖက်\n",
      "By:\n",
      "ယင်းယဉ်ကျော်\n",
      "ပန်းပွင့်ကာ ၊ မချမ်းသာသည်\n",
      "စကားသစ္စာ ၊ အရိပ်အင်သည်\n",
      "ဝမ်းစာသည်အို ၊ ပြည့်ရင်ဟု\n",
      "တိမ်ညိုပေါ် ၊ ချစ်စိတ်ကောင်းကို\n",
      "ချစ်သူ့နှင်းဆီ ၊ သပြေညီနှင့်\n",
      "ရှုဖျန်းသိမ်းကြည် ၊ ပီတိထွေလည်း\n",
      "သောကနှင့် ၊ စေတီ ဆီးကြည့်ဝ\n",
      "ချစ်ကြည်သာမွေ ၊ ယုံကြည်လွင်နှံ့သို့\n",
      "ကြည်မွေ့လှပါ ၊ ကြည်လင်နှုတ်မှ\n",
      "ဟောင်းအောက် ကိုယ်စီ ၊ အို မေ့သည်\n",
      "ရွေ့ရွှေ့သာလျှင်\n",
      "လွမ်းရင်ထွေး ၊ ချ\n",
      "---------------\n",
      "\n",
      "သူတို့ဖြစ်တည်ငြား ။\n",
      "\n",
      "Title: ဝန်တို့ရွှေရည်\n",
      "By: မြေလတ်မင်းလွင်\n",
      "မြးစည်ဘက်သို့ စက်ဝိုင်းဝိုင်း\n",
      "ရှုတိုင်းတောင်ရိုက် ။\n",
      "တွယ်တာ ရွယ်ရာမှိုင်း\n",
      "မိုးလျားဝါ ဝန်တာ ။\n",
      "ကျွန်တော်ခြည်ကို သူနှောပါဟူ\n",
      "တောမိုးရေ ၊ အိုအိုရေလို့\n",
      "ပြာပါရော သူ့အမျိုး ။\n",
      "အိမ်ပြေ စည်မောက်လျှင်\n",
      "တောက်ချိုရွှင် လှည့်ပတ်လွှတ် ။\n",
      "\n",
      "Title: စည်မြေ မြေဝသည်\n",
      "By: မြေလတ်မင်းလွင်\n",
      "ကျိုးမျဉ်းမြောက်စေသည်\n",
      "တောရစ်မောက်လွင့် ပျောက်ရောက်ပါတကား ။\n",
      "ဝန်းကျင်ရစေဖြစ်သောအခါမှ ၊ ရှုထောက်ကျစ်မည့်\n",
      "အမွေးမပျက် ၊ ရွှေရည်လက်ကာ\n",
      "ထက်မြွက်ခဲ့လိုက် ၊ ယူလိုက်မည့်အညာ ။\n",
      "တောင်စွယ်တောင\n",
      "---------------\n",
      "\n",
      "ပြည်သူ့အမျိုး\n",
      "ငြိမ်းချမ်းရေး မျက်ရည်မျှော်လင်\n",
      "အလင်းရေးသီ တပ်ဆင်းပါလိမ့်\n",
      "ခင်မငြင်းမငြီး အပင်းအလွှာမှာဖြင့်\n",
      "သမုဒယပန်း ဆင်းရဲသွေးစုံနှင့်\n",
      "ဘဝတွေ စုံလင်လွှမ်းရအောင်\n",
      "မာနမှု အပြစ်တွေပ\n",
      "ထွေထွေ တောအရပ်မှာဖြင့်\n",
      "တပါးဆီကုန် ။\n",
      "စာပေ အခင်းအဝေးကိုဖြင့်\n",
      "သာသနာ ဝေးဝေး အခြေခံထားတော့\n",
      "အထင်အရေးနှင့် မရပါအောင်\n",
      "တပည့် ခြေမလွှမ်း ။\n",
      "စန်းသာမှု မကျန်ဘု\n",
      "ဆရာဝင် လွမ်းရတော့\n",
      "ဒုက္ခသည် ချစ်သူရယ်\n",
      "တည်တစ်ပင် သာလွမ်းစေကြောင့်\n",
      "ဝမ်းမြင်မြင် စိတ်ထားဝါခြုံလှ\n",
      "မြင်ပြုံးပျံ့ကြွယ် အသွယ်သွယ်နှံ့ကွယ်ထွန်းမြ\n",
      "ပြုံးကြ၏ နယ်ခွင့်လော့\n",
      "နယ်ခွင့်လှည်း မို\n",
      "---------------\n",
      "\n",
      "real\t0m21.655s\n",
      "user\t0m19.171s\n",
      "sys\t0m5.347s\n"
     ]
    }
   ],
   "source": [
    "!time python -m torch.distributed.launch --use-env sample.py --out_dir=out-kabyar-char | tee test_kabyar_char.dist.log1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing-2 (char Model)\n",
    "I changed seed value to \"2023\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rnd/anaconda3/envs/nanoGPT/lib/python3.8/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use-env is set by default in torchrun.\n",
      "If your script expects `--local-rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  warnings.warn(\n",
      "Overriding: out_dir = out-kabyar-char\n",
      "number of parameters: 10.68M\n",
      "Loading meta from data/kabyar_char/meta.pkl...\n",
      "\n",
      "သူတို့ယောက်ျား ။\n",
      "သူတို့ကိုသာ လိုညာသည်\n",
      "လှောက်စရာ ကြိုးစား ။\n",
      "ကိုယ်တို့ကြည့်ရှောက်ဖို့ မှောင်သား ။\n",
      "သွား သူတော် ငှက်ကို သူ အုတ်အုပ်ထွေ\n",
      "ပျော်ရွှင် ရောင်ခြည်မှု ။\n",
      "ငှက်ကျား ပျို့ရည် ကိုယ့်မှာခ\n",
      "ငှက်ငယ် ရှုတ်မပါ ။\n",
      "သူတို့ကို ပြန်ပေါကြ\n",
      "ကိုယ့်တို့ဆီ ချိုချိုပြန် ။\n",
      "သူ့မျက်နှာ ပျင်းတော့\n",
      "နှစ်ပြန်ပေါ် သက်ရောင်ထွက် ။\n",
      "သူ့မျက်ရည်\n",
      "သွေးစက်တော့ တောင့်တောက် ။\n",
      "သူ့ကိုယ့် သူတို့နှင့်\n",
      "ဘယ်သူ ဘာမျက်နှာ ဖြစ်ပေါက်\n",
      "ကြည်ဖြူစင်စင် ။\n",
      "\n",
      "Title: မိုးစက်ပွင့်မင်\n",
      "By: မင်းသုဝဏ်\n",
      "ရွေးချင်ရည်ရွှေစက်၊ လေချင်းပျက်စေ့\n",
      "ရွက်ဆွတ်ဖက် ၊ ကျွန်းပက်သ\n",
      "---------------\n",
      "\n",
      "တောမှာမှာအနှံ့\n",
      "ယွင်းပြုံးယံ ၊ ရုတ်ရုံပြင်ခြွေ\n",
      "မေခေဝါးပြည်ရိုး ၊ ထူးပြည်ရိုးသားကို\n",
      "မေတ္တာမိုးရိုး ၊ သစ္စာချေဖိုးဖြင့်\n",
      "ရှုလည်းရှုးဆက်ပါတကား ။\n",
      "သုခုမှတ်\n",
      "အံ့မခံယူ ၊ အို\n",
      "ဖြူဖြူစင် ၊ အမေချိန်ဖြူ၍\n",
      "ကြီးကားရှင်အံ့ ၊ စွမ်းရည်လျှင် ။\n",
      "လွမ်းဆက်ကာ\n",
      "တစ်နေ့တစ်သွေး၊ ပျစ်ရာရှာတို့\n",
      "သမုဒယဝယ် ၊ အတွင်းတည်း ။\n",
      "အမောဟရာမင် ၊ ငါတို့မင်းသည်\n",
      "ကာလကြင်နာခွင် ၊ လိုရာစရိုက်ကို\n",
      "ဆိုရှိလျှင် ၊ နှလုံးလွင်၍\n",
      "လွမ်းဆင်ခြင်း ၊ စိတ်မင်းရင်းလျှင်\n",
      "ထိန်ပင်တိမ် ၊ အစဉ်တိမ်ခင်းကား\n",
      "တိမ်ဇွဲမြဲမိ ၊ နှုတ်ဆက်ဖိတ်လွှဲ ။\n",
      "\n",
      "Title: မိုးရေးသား\n",
      "By: မင်းသုဝဏ်\n",
      "---------------\n",
      "\n",
      "နယ်ခွာကို ။\n",
      "မြန်မာ အမြင့်မာဘနှင့်\n",
      "သဘာဝရဲ့ သဘာဝမှာ\n",
      "ဘဝအား တရားမာန်ခွာလို့\n",
      "လျှံ ဓလေ့အင် ။\n",
      "ပုံသဏ္ဍာ စံတင့်ရယ်နှင့်\n",
      "ထူးစွမ်းသည့် ဆွေပြောင်းကပ်မှာဖြင့်\n",
      "တွေးသွန်းပူဆွေ ။\n",
      "\n",
      "Title: မြန်မာ့ဓလေ့ အနှံ့\n",
      "By: မြေလတ်မင်းလွင်\n",
      "ချစ်စရာ ဘဝမြတ်နိုး\n",
      "ထာဝရ ဝန်းကံကွန်းကျင်\n",
      "စံပယ်တိုးအေး ။\n",
      "မြန်မာ ဘဝတွင်\n",
      "အစဉ်တိုးထိုး ဖျားပွားစေ ။\n",
      "ကြိုးပမာ အနာမြန်စွာ\n",
      "စာဆိုးမျိုးအလှ ဝန်းကျင်နွှဲ ။\n",
      "ငှက်ကလေး မြုပ်အစဉ် ။\n",
      "\n",
      "Title: သယ်ခရီး\n",
      "By: မောင်စိုးမြတ် (ချောင်းဦး)\n",
      "အချိန်တွေက အစဉ်ရှာတဲ့\n",
      "ဓလေ့ရှာဖွေ နားမထွက်နိုင်သေးရင်\n",
      "လောကဓံတွေက ကွက်ကွက်ကွက်ကြပါ\n",
      "---------------\n",
      "\n",
      "နှစ်ယောက် ၊ ဆယ်သည်ယား\n",
      "လိုရာဂုဏ် ၊ ခရီးဝန်းကျင်သည်\n",
      "ဖူးစာသင်ပုံ ၊ သေပုံရိပ်\n",
      "ရှုမြုံချမ်း\n",
      "ရိပ်မြုံပန်း ၊ မျှော်လင်းဖြိုးဖြိုး ။\n",
      "ထူးဆန်းရိပ်မြုံ ၊ သည်အထုံလည်း\n",
      "အရုဏ်ပြုံးဝေ ၊ သည်အလှဆင်၍\n",
      "အလှပွေ့ဆီ ၊ အလှဆင်ကို\n",
      "ရင်သီကာချီတက် စနစ်လက်တွယ် ၊ တစ်ကျော်ကြ\n",
      "ဆော်တက်ရည်လျှံ ၊ မိုးနန်းဖွယ်ဖြင့်\n",
      "တို့မြန်းရွှင်ကြည် ၊ ချစ်ကျွန်းမြူစွ ။\n",
      "ဆောင်းရိပ်အား ၊ တို့မြေတောင့်စွာ\n",
      "တည်ငြိမ်သာပျော်ပါလေ တဲ့ ။\n",
      "\n",
      "Title: ထာဝရပန်း ဘာဇာတ်စုံ တို့မြင်မွှေးရနံ့ ပျံ့ထုံမွှေး အလှဖွဲ့မြှင့်ပေးခြင်း\n",
      "By: မြေလတ်မင်းလွင်\n",
      "တိမ်ညိုညိုလောင်းရော် , ချစ\n",
      "---------------\n",
      "\n",
      "သူ့ကိုယ်ကိုယ်က\n",
      "ဘာကြောင့်အဖော် သူကစားနဲ့\n",
      "သူ့ကိုယ်ကတော့\n",
      "ကိုယ်သူ အနာဂတ်ကို\n",
      "သိမ်းသွားရတာတော့ပါပဲ။\n",
      "သူ့အမေ ဘာကြောင့်\n",
      "စိတ်ကူးပြောပါ\n",
      "အမေ ကြောင်းကြည့်\n",
      "စိတ်တွေ အရက်စက်လေတော့\n",
      "စွဲစွပ်မွေ့လှပါ့မယ်လေ\n",
      "အမေ ရှေ့ကာပျက်ပါစေ\n",
      "ဟေ့ တစ်မောင်စစ်ခွင်\n",
      "တစ်ခွင်ပျက်ခွင်\n",
      "တို့တစ်ယောက်ကိုယ်တွင်လို့\n",
      "အစဉ်ထင်ထောင်ပွဲတွေက\n",
      "ရှေ့တောင်ကွယ်ရာ ...\n",
      "ထာဝရ\n",
      "တို့ပြည်ရွာဖြစ်ခဲ့ပြီကောင်း\n",
      "ခက်သည်နေစဉ် ... ။\n",
      "\n",
      "Title: အရိပ်\n",
      "By: ဇာနည်\n",
      "အံ့အားကြည်လင်၊ တိမ်ညွန့်ပြည်အားထုံးလျက်\n",
      "တိမ်ရှုံးသစ်ရွက်စာ၊ ရောင်ဝယ်ယာသို့\n",
      "လက်တစ်ခွက်နုရွ၊ ပြည့်ဝန်းကျ၍\n",
      "အခွက်ပြကုန်း၊ \n",
      "---------------\n",
      "\n",
      "ရောင်သူကြိုးကြိုးနှင့်။ ။\n",
      "\n",
      "Title: တေးသံနှင့် မိုးရွာမြေ\n",
      "By: မြင်းမူမောင်နိုင်မိုး\n",
      "တောသို့သည် မိုးရွာလိုများ\n",
      "မိုးကောင်းသည် အိုအကျေးရှင်\n",
      "ထိုဆုံးပြင်တဲ့ မိုးရွာ။\n",
      "တောမိုးရွာလယ် မိုးရွာကို\n",
      "အိုစွယ်တာ အနေခက်\n",
      "ကျေးဇူးမှသာ။\n",
      "ဟော ...\n",
      "ရှောင်တန်ဖိုး မျိုးလေပြင်။\n",
      "မိုးရွာခြင်းကို အဟုတ်မပြတ်\n",
      "စိတ်မပြတ်မျှော်ငြား။\n",
      "ရှောင်ဖို့မလွန်\n",
      "မြင့်မြင်သမျှတ အစဉ်လာထင်။\n",
      "မြင့်မားဖို့ မြတ်စို့\n",
      "အောင်ပြုတတ်သော အားအင်အား\n",
      "ထိုင်ရှားလေပြီ။\n",
      "ပင်လယ်သမား သဘာဝမှ\n",
      "နားဆင်ခြင်ပါ...။\n",
      "ရှုခင်ကား ပေါင်းသား\n",
      "အနားတွေးတွေ တကွေး\n",
      "ရှင်သနားမ ထားရပါ။\n",
      "အစာလှပါ\n",
      "---------------\n",
      "\n",
      "စစ်အချစ်ဆိုတာ မတဲ့\n",
      "အချစ်ရယ် သူငယ်ချင်း မငြင်းချင်း\n",
      "သူကောင်းတို့ အလင်းကျွေး ထိန်းထားတဲ့\n",
      "ကြည့်နေတဲ့မြတ်လွင်အားလုံး မျဉ်းကျဉ်းကလေးရယ်\n",
      "ကြည့်လိုက်တဲ့စိတ်နေတဲ့ အရသာ အလွယ်ကို\n",
      "ကျွန်တော့်အတိုင်း\n",
      "ကျွန်တော့်အလွန်တော့မယ် ။\n",
      "ဘယ်မှာပေါင်းကို ဖွံ့ဖြိုးနေပါတယ် ။\n",
      "ချစ်သူဆိုတာ\n",
      "ဘယ်သူမှ အလွမ်းခွဲဖြစ်နေတဲ့\n",
      "ကျွန်တော့်ကျွန်တော့်အကြည် ဖြစ်နိုင်ရတဲ့\n",
      "ကဗျာဆရာ\n",
      "ကဗျာတစ်ယောက်က တစ်ယောက်တည်းသေးတာ နေ့က\n",
      "ကျွန်တော်တို့ မကျွန်တော်တို့ရဲ့ တောက်တိုက်တယ်\n",
      "ကျွန်တော့်လက်ကို အောက်ကျွန်တော့်စားတယ် ။\n",
      "ကိုယ့်အဖေအကြောင်း တစ်ယောက်တစ်ယောက်\n",
      "ကျွန်တော\n",
      "---------------\n",
      "\n",
      "မျက်ရည်ကြည်ရဲ့ တစ်နေ့\n",
      "ကျနော် ဖြောင့်နေရသလို\n",
      "ခန္ဓာကို နှောင့်ကိုချင်ရဲ့\n",
      "အားလုံးလို ကြုံသည်\n",
      "ကျနော့်တစ်ခွင်၊ မျက်နှာပင်လယ်ဖြင့်\n",
      "အမှောင်လမ်းထက်၊ တစ်နေ့တစ်ဝက်\n",
      "အားရှက်မရနိုင်ခြင်း၊ ရင်ခုန်ရိုးရှိ\n",
      "ကြင်နာမိတ္တာ၊ စိတ်ပျော်ပွားနေတဲ့\n",
      "အလှမရှိတဲ့ ဘဝအနှစ်မှာ\n",
      "သစ္စာသစ္စာ၊ ရှင်သနာအသစ်\n",
      "အလှတရားဆိုရင်\n",
      "သူ့ရဲ့ အလှပြလိမ့်မယ်။\n",
      "\n",
      "Title: အရိပ်ရဲ့ လွမ်းချစ်ခြင်း\n",
      "By: ညွန့်ဝေ\n",
      "ပန်းပွင့်လိုက်ပါ\n",
      "ယိုင်လို့လဲပါသလား\n",
      "စိတ်လျှင်ပွင့်ဖူး၊ အလှည့်သက်ဆုံးခဲ\n",
      "လှမ်းဆက်ရေးရှု၊ ပန်းစုလျှံဝေး\n",
      "လေဝေးလှမ်းခဲ့ရသည်။\n",
      "အခါပြင်မတွေး၊ အနေပြင်ကျင့်\n",
      "ဖွံ့ဖ\n",
      "---------------\n",
      "\n",
      "မြို့ကြိုးလို့ မြန်မာစွန်းမြတ်ကိုဖြင့်\n",
      "ဘဝတွေ ပြည်သူအပွင့်ဖြင့်\n",
      "တရားတွက်ပြည့်ဝင့် တင့်တယ်\n",
      "မြန်မာ့အလှ ထိုမြင့်ပါလှခဲ့\n",
      "ဥက္ကဋ္ဌသည် ရတနာတို့\n",
      "ဓမ္မစည် ရိပ်မြုံလွှမ်းတို့\n",
      "ဓမ္မနံ့ တင့်တယ်လှပြီ\n",
      "တင့်တယ်လှတော် ထိုဝယ်ထိုရယ်\n",
      "ချစ်ခြင်းမြတ်နောက်ကွာ မြနန်းမာ့သောက်မှုထံ\n",
      "မဆုံးမြန်ဘုံ သရက်သွယ်ဖို့\n",
      "မျှဝေပင် တပ်ဆုံဓလေ့သော်\n",
      "လူ့ယုံကြည်ကြည် နယ်ချဲ့မျှော်မြင်သော\n",
      "သူယုဆယ်ရာ ထက်ကောင်းကျွေးသော\n",
      "ကောင်းသောအခါ အပါမည် ။\n",
      "ထိုနေရာ ရိပ်သာသည်\n",
      "ရိပ် ခရီးသာလျှင်\n",
      "မျှော်ရှိနေရာ သူကောင်းညာ ။\n",
      "မိုးသုံးပေါ်တန်း ကြီးနန်းတွင်\n",
      "ထကြွေဆင်ရော်မြင် ထိ\n",
      "---------------\n",
      "\n",
      "ချမ်းမြတွေ ပါရမီဖြည့်တကား ။\n",
      "\n",
      "Title: မိုးအမေ\n",
      "By: မောင်စိုးမြတ် (ချောင်းဦး)\n",
      "မယ်တို့မျက်ရည်ထင် ၊ အစဉ်လာတွင်\n",
      "မြတ်နိုးရပ်လေး ။\n",
      "ဘယ်နှယ်မှာ ၊ ဘယ်သို့မောကား\n",
      "အလှဟာကွယ် ၊ ချွေးနွယ်ကျယ်မှာ\n",
      "လှတယ်လေး ။\n",
      "မယ်စိမ်းအိမ်လေး ၊ ရွှေပြည်လေး ။\n",
      "လှတယ်လေး ၊ မယ်လေး ။\n",
      "ပတ်လွှတ်လပ် ၊ ဟားတိမ်က ။\n",
      "လှမ်းရရောင်ဝယ်\n",
      "လှမ်းမိဆ ၊ လှမ်းရရောင်ပစ်\n",
      "လှမ်းရအောင်မှာ ရွှန်းရောင်ဖြာ ၊ နေခြည်တို့\n",
      "လက်တရား ၊ မောဟမသာ ။\n",
      "ယင်းယောင်းလေး ၊ မောဖွေးရာဟု\n",
      "ရွာလယ်တွင် ၊ မောဟမှန်ရောင်လျက်\n",
      "ရှုမော ပြောလွှမ်း ၊ လှတည့်မသွမ်းလျက်\n",
      "နွမ်းရည်ဆွတ်ဖွယ် ၊ ခရီးရည်တို့\n",
      "နွမ\n",
      "---------------\n",
      "\n",
      "real\t0m21.657s\n",
      "user\t0m19.178s\n",
      "sys\t0m5.399s\n"
     ]
    }
   ],
   "source": [
    "!time python -m torch.distributed.launch --use-env sample.py --out_dir=out-kabyar-char | tee test_kabyar_char.dist.log2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation for BPE Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rnd/tool/nanoGPT/data/kabyar_bpe\n"
     ]
    }
   ],
   "source": [
    "%cd data/kabyar_bpe/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\n",
      "import requests\n",
      "import tiktoken\n",
      "import numpy as np\n",
      "\n",
      "# download the tiny shakespeare dataset\n",
      "input_file_path = os.path.join(os.path.dirname(__file__), 'kabyar-corpus-ver1.0.txt')\n",
      "\n",
      "with open(input_file_path, 'r') as f:\n",
      "    data = f.read()\n",
      "n = len(data)\n",
      "train_data = data[:int(n*0.9)]\n",
      "val_data = data[int(n*0.9):]\n",
      "\n",
      "# encode with tiktoken gpt2 bpe\n",
      "enc = tiktoken.get_encoding(\"gpt2\")\n",
      "train_ids = enc.encode_ordinary(train_data)\n",
      "val_ids = enc.encode_ordinary(val_data)\n",
      "print(f\"train has {len(train_ids):,} tokens\")\n",
      "print(f\"val has {len(val_ids):,} tokens\")\n",
      "\n",
      "# export to bin files\n",
      "train_ids = np.array(train_ids, dtype=np.uint16)\n",
      "val_ids = np.array(val_ids, dtype=np.uint16)\n",
      "train_ids.tofile(os.path.join(os.path.dirname(__file__), 'train.bin'))\n",
      "val_ids.tofile(os.path.join(os.path.dirname(__file__), 'val.bin'))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat ./prepare-bpe.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run above python script**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train has 3,040,974 tokens\n",
      "val has 345,016 tokens\n"
     ]
    }
   ],
   "source": [
    "!python ./prepare-bpe.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config File Preparation for BPE Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rnd/tool/nanoGPT/config\n"
     ]
    }
   ],
   "source": [
    "%cd /home/rnd/tool/nanoGPT/config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-kabyar-bpe'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 200\n",
      "log_interval = 10 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'kabyar-bpe'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'kabyar_bpe'\n",
      "batch_size = 32\n",
      "block_size = 256 # context of up to 256 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 6\n",
      "n_head = 6\n",
      "n_embd = 384\n",
      "dropout = 0.2\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 5000\n",
      "lr_decay_iters = 5000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n"
     ]
    }
   ],
   "source": [
    "!cat ./train_kabyar_bpe.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building GPT-2 Model with BPE Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rnd/tool/nanoGPT\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rnd/anaconda3/envs/nanoGPT/lib/python3.8/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use-env is set by default in torchrun.\n",
      "If your script expects `--local-rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  warnings.warn(\n",
      "Overriding config with ./config/train_kabyar_bpe.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-kabyar-bpe'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 200\n",
      "log_interval = 10 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'kabyar-bpe'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'kabyar_bpe'\n",
      "batch_size = 32\n",
      "block_size = 256 # context of up to 256 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 6\n",
      "n_head = 6\n",
      "n_embd = 384\n",
      "dropout = 0.2\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 5000\n",
      "lr_decay_iters = 5000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "total number of tokens per iteration: 40960\n",
      "Initializing a new model from scratch\n",
      "defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\n",
      "number of parameters: 29.94M\n",
      "using fused AdamW: True\n",
      "compiling the model... (takes a ~minute)\n",
      "step 0: train loss 10.9087, val loss 10.9073\n",
      "[2023-04-18 04:28:46,479] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-18 04:28:47,260] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-18 04:28:47,921] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-18 04:28:48,250] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-18 04:28:48,567] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-18 04:28:48,895] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-18 04:28:49,212] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-18 04:28:49,540] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-18 04:28:49,856] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-18 04:28:50,183] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-18 04:28:50,502] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-18 04:28:50,953] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "iter 0: loss 10.8982, time 23784.62ms, mfu -100.00%\n",
      "iter 10: loss 6.9973, time 359.36ms, mfu 6.82%\n",
      "iter 20: loss 5.2244, time 361.03ms, mfu 6.82%\n",
      "iter 30: loss 3.7593, time 360.55ms, mfu 6.82%\n",
      "iter 40: loss 2.2006, time 363.23ms, mfu 6.81%\n",
      "iter 50: loss 1.4884, time 360.82ms, mfu 6.81%\n",
      "iter 60: loss 1.3669, time 365.26ms, mfu 6.80%\n",
      "iter 70: loss 1.3269, time 363.52ms, mfu 6.79%\n",
      "iter 80: loss 1.3175, time 363.96ms, mfu 6.79%\n",
      "iter 90: loss 1.3115, time 361.78ms, mfu 6.79%\n",
      "iter 100: loss 1.2947, time 363.49ms, mfu 6.78%\n",
      "iter 110: loss 1.2831, time 363.80ms, mfu 6.78%\n",
      "iter 120: loss 1.3200, time 362.78ms, mfu 6.77%\n",
      "iter 130: loss 1.3192, time 364.53ms, mfu 6.77%\n",
      "iter 140: loss 1.3132, time 363.33ms, mfu 6.77%\n",
      "iter 150: loss 1.3242, time 363.59ms, mfu 6.76%\n",
      "iter 160: loss 1.3047, time 363.96ms, mfu 6.76%\n",
      "iter 170: loss 1.3144, time 363.66ms, mfu 6.76%\n",
      "iter 180: loss 1.3124, time 364.59ms, mfu 6.76%\n",
      "iter 190: loss 1.2876, time 363.87ms, mfu 6.75%\n",
      "iter 200: loss 1.2861, time 364.85ms, mfu 6.75%\n",
      "iter 210: loss 1.2543, time 368.13ms, mfu 6.74%\n",
      "iter 220: loss 1.2516, time 368.46ms, mfu 6.73%\n",
      "iter 230: loss 1.2322, time 370.15ms, mfu 6.72%\n",
      "iter 240: loss 1.2310, time 364.90ms, mfu 6.72%\n",
      "step 250: train loss 1.2209, val loss 1.2148\n",
      "saving checkpoint to out-kabyar-bpe\n",
      "iter 250: loss 1.2399, time 14116.63ms, mfu 6.07%\n",
      "iter 260: loss 1.2256, time 368.76ms, mfu 6.12%\n",
      "iter 270: loss 1.2100, time 370.49ms, mfu 6.17%\n",
      "iter 280: loss 1.1854, time 372.83ms, mfu 6.21%\n",
      "iter 290: loss 1.1745, time 370.57ms, mfu 6.25%\n",
      "iter 300: loss 1.1321, time 371.59ms, mfu 6.29%\n",
      "iter 310: loss 1.1328, time 367.26ms, mfu 6.33%\n",
      "iter 320: loss 1.0728, time 372.00ms, mfu 6.35%\n",
      "iter 330: loss 1.0454, time 364.68ms, mfu 6.39%\n",
      "iter 340: loss 1.0305, time 365.78ms, mfu 6.42%\n",
      "iter 350: loss 0.9571, time 366.67ms, mfu 6.45%\n",
      "iter 360: loss 0.9759, time 367.83ms, mfu 6.47%\n",
      "iter 370: loss 0.9598, time 367.37ms, mfu 6.49%\n",
      "iter 380: loss 0.9443, time 367.19ms, mfu 6.51%\n",
      "iter 390: loss 0.9131, time 367.44ms, mfu 6.52%\n",
      "iter 400: loss 0.9127, time 369.08ms, mfu 6.54%\n",
      "iter 410: loss 0.9074, time 364.63ms, mfu 6.55%\n",
      "iter 420: loss 0.8982, time 364.21ms, mfu 6.57%\n",
      "iter 430: loss 0.8996, time 363.90ms, mfu 6.59%\n",
      "iter 440: loss 0.9025, time 366.98ms, mfu 6.60%\n",
      "iter 450: loss 0.8856, time 367.62ms, mfu 6.60%\n",
      "iter 460: loss 0.8752, time 367.28ms, mfu 6.61%\n",
      "iter 470: loss 0.8483, time 367.63ms, mfu 6.62%\n",
      "iter 480: loss 0.8593, time 372.13ms, mfu 6.61%\n",
      "iter 490: loss 0.8612, time 371.20ms, mfu 6.61%\n",
      "step 500: train loss 0.8254, val loss 0.8230\n",
      "saving checkpoint to out-kabyar-bpe\n",
      "iter 500: loss 0.8910, time 14742.53ms, mfu 5.97%\n",
      "iter 510: loss 0.8640, time 364.66ms, mfu 6.04%\n",
      "iter 520: loss 0.8578, time 365.74ms, mfu 6.11%\n",
      "iter 530: loss 0.8337, time 366.82ms, mfu 6.17%\n",
      "iter 540: loss 0.8433, time 364.59ms, mfu 6.22%\n",
      "iter 550: loss 0.8325, time 365.19ms, mfu 6.27%\n",
      "iter 560: loss 0.8126, time 367.06ms, mfu 6.31%\n",
      "iter 570: loss 0.8225, time 371.44ms, mfu 6.34%\n",
      "iter 580: loss 0.8181, time 365.06ms, mfu 6.38%\n",
      "iter 590: loss 0.8340, time 364.71ms, mfu 6.41%\n",
      "iter 600: loss 0.8105, time 364.23ms, mfu 6.44%\n",
      "iter 610: loss 0.7888, time 366.46ms, mfu 6.47%\n",
      "iter 620: loss 0.7976, time 369.29ms, mfu 6.49%\n",
      "iter 630: loss 0.8235, time 368.16ms, mfu 6.50%\n",
      "iter 640: loss 0.8168, time 367.83ms, mfu 6.52%\n",
      "iter 650: loss 0.7937, time 371.89ms, mfu 6.53%\n",
      "iter 660: loss 0.7851, time 366.35ms, mfu 6.54%\n",
      "iter 670: loss 0.8003, time 372.96ms, mfu 6.55%\n",
      "iter 680: loss 0.7882, time 370.46ms, mfu 6.55%\n",
      "iter 690: loss 0.7650, time 373.08ms, mfu 6.55%\n",
      "iter 700: loss 0.7780, time 366.03ms, mfu 6.57%\n",
      "iter 710: loss 0.8145, time 363.65ms, mfu 6.59%\n",
      "iter 720: loss 0.7635, time 367.03ms, mfu 6.60%\n",
      "iter 730: loss 0.7897, time 370.20ms, mfu 6.60%\n",
      "iter 740: loss 0.7881, time 365.24ms, mfu 6.61%\n",
      "step 750: train loss 0.7420, val loss 0.7500\n",
      "saving checkpoint to out-kabyar-bpe\n",
      "iter 750: loss 0.7672, time 13467.62ms, mfu 5.97%\n",
      "iter 760: loss 0.7716, time 365.86ms, mfu 6.04%\n",
      "iter 770: loss 0.7617, time 365.56ms, mfu 6.11%\n",
      "iter 780: loss 0.7597, time 368.97ms, mfu 6.16%\n",
      "iter 790: loss 0.7487, time 364.21ms, mfu 6.22%\n",
      "iter 800: loss 0.7468, time 363.42ms, mfu 6.27%\n",
      "iter 810: loss 0.7445, time 365.76ms, mfu 6.31%\n",
      "iter 820: loss 0.7642, time 367.63ms, mfu 6.35%\n",
      "iter 830: loss 0.7508, time 366.05ms, mfu 6.38%\n",
      "iter 840: loss 0.7434, time 370.50ms, mfu 6.41%\n",
      "iter 850: loss 0.7314, time 368.38ms, mfu 6.43%\n",
      "iter 860: loss 0.7507, time 363.23ms, mfu 6.46%\n",
      "iter 870: loss 0.7422, time 365.37ms, mfu 6.49%\n",
      "iter 880: loss 0.7695, time 364.86ms, mfu 6.51%\n",
      "iter 890: loss 0.7479, time 364.81ms, mfu 6.53%\n",
      "iter 900: loss 0.7362, time 365.15ms, mfu 6.55%\n",
      "iter 910: loss 0.7172, time 365.20ms, mfu 6.57%\n",
      "iter 920: loss 0.7347, time 373.46ms, mfu 6.57%\n",
      "iter 930: loss 0.7240, time 374.58ms, mfu 6.56%\n",
      "iter 940: loss 0.7150, time 365.80ms, mfu 6.58%\n",
      "iter 950: loss 0.7086, time 365.50ms, mfu 6.59%\n",
      "iter 960: loss 0.7399, time 364.24ms, mfu 6.60%\n",
      "iter 970: loss 0.7183, time 365.01ms, mfu 6.62%\n",
      "iter 980: loss 0.7453, time 371.62ms, mfu 6.61%\n",
      "iter 990: loss 0.7084, time 369.67ms, mfu 6.61%\n",
      "step 1000: train loss 0.6915, val loss 0.7047\n",
      "saving checkpoint to out-kabyar-bpe\n",
      "iter 1000: loss 0.7120, time 13006.54ms, mfu 5.97%\n",
      "iter 1010: loss 0.7255, time 369.40ms, mfu 6.04%\n",
      "iter 1020: loss 0.7220, time 367.00ms, mfu 6.10%\n",
      "iter 1030: loss 0.7183, time 368.68ms, mfu 6.16%\n",
      "iter 1040: loss 0.7128, time 366.80ms, mfu 6.21%\n",
      "iter 1050: loss 0.6771, time 365.49ms, mfu 6.26%\n",
      "iter 1060: loss 0.7307, time 365.78ms, mfu 6.30%\n",
      "iter 1070: loss 0.7211, time 366.50ms, mfu 6.34%\n",
      "iter 1080: loss 0.6968, time 367.67ms, mfu 6.37%\n",
      "iter 1090: loss 0.6775, time 372.37ms, mfu 6.40%\n",
      "iter 1100: loss 0.7237, time 367.13ms, mfu 6.42%\n",
      "iter 1110: loss 0.6762, time 364.55ms, mfu 6.45%\n",
      "iter 1120: loss 0.7081, time 363.98ms, mfu 6.48%\n",
      "iter 1130: loss 0.7186, time 364.25ms, mfu 6.51%\n",
      "iter 1140: loss 0.6939, time 367.66ms, mfu 6.52%\n",
      "iter 1150: loss 0.6912, time 367.05ms, mfu 6.54%\n",
      "iter 1160: loss 0.6874, time 371.87ms, mfu 6.54%\n",
      "iter 1170: loss 0.6954, time 369.30ms, mfu 6.55%\n",
      "iter 1180: loss 0.7125, time 371.90ms, mfu 6.56%\n",
      "iter 1190: loss 0.6723, time 369.68ms, mfu 6.56%\n",
      "iter 1200: loss 0.6777, time 373.19ms, mfu 6.56%\n",
      "iter 1210: loss 0.6939, time 366.27ms, mfu 6.58%\n",
      "iter 1220: loss 0.6835, time 373.98ms, mfu 6.57%\n",
      "iter 1230: loss 0.6829, time 372.53ms, mfu 6.58%\n",
      "iter 1240: loss 0.7070, time 374.97ms, mfu 6.57%\n",
      "step 1250: train loss 0.6557, val loss 0.6800\n",
      "saving checkpoint to out-kabyar-bpe\n",
      "iter 1250: loss 0.6820, time 12757.70ms, mfu 5.93%\n",
      "iter 1260: loss 0.6550, time 364.87ms, mfu 6.01%\n",
      "iter 1270: loss 0.7031, time 367.71ms, mfu 6.08%\n",
      "iter 1280: loss 0.6754, time 365.78ms, mfu 6.14%\n",
      "iter 1290: loss 0.6907, time 366.36ms, mfu 6.19%\n",
      "iter 1300: loss 0.6753, time 373.35ms, mfu 6.23%\n",
      "iter 1310: loss 0.6696, time 373.36ms, mfu 6.27%\n",
      "iter 1320: loss 0.6823, time 364.36ms, mfu 6.31%\n",
      "iter 1330: loss 0.6644, time 364.00ms, mfu 6.35%\n",
      "iter 1340: loss 0.6692, time 363.68ms, mfu 6.39%\n",
      "iter 1350: loss 0.6692, time 367.00ms, mfu 6.42%\n",
      "iter 1360: loss 0.6586, time 371.13ms, mfu 6.44%\n",
      "iter 1370: loss 0.6603, time 372.15ms, mfu 6.45%\n",
      "iter 1380: loss 0.6734, time 371.96ms, mfu 6.47%\n",
      "iter 1390: loss 0.6680, time 369.70ms, mfu 6.48%\n",
      "iter 1400: loss 0.6760, time 367.15ms, mfu 6.50%\n",
      "iter 1410: loss 0.6618, time 364.03ms, mfu 6.53%\n",
      "iter 1420: loss 0.6419, time 363.96ms, mfu 6.55%\n",
      "iter 1430: loss 0.6464, time 368.55ms, mfu 6.56%\n",
      "iter 1440: loss 0.6744, time 366.17ms, mfu 6.57%\n",
      "iter 1450: loss 0.6705, time 370.12ms, mfu 6.58%\n",
      "iter 1460: loss 0.6448, time 367.34ms, mfu 6.59%\n",
      "iter 1470: loss 0.6566, time 368.30ms, mfu 6.59%\n",
      "iter 1480: loss 0.6701, time 372.86ms, mfu 6.59%\n",
      "iter 1490: loss 0.6831, time 368.82ms, mfu 6.60%\n",
      "step 1500: train loss 0.6319, val loss 0.6633\n",
      "saving checkpoint to out-kabyar-bpe\n",
      "iter 1500: loss 0.6673, time 12992.46ms, mfu 5.96%\n",
      "iter 1510: loss 0.6650, time 370.82ms, mfu 6.02%\n",
      "iter 1520: loss 0.6643, time 372.61ms, mfu 6.08%\n",
      "iter 1530: loss 0.6289, time 371.53ms, mfu 6.13%\n",
      "iter 1540: loss 0.6569, time 371.98ms, mfu 6.17%\n",
      "iter 1550: loss 0.6408, time 371.38ms, mfu 6.22%\n",
      "iter 1560: loss 0.6673, time 368.38ms, mfu 6.26%\n",
      "iter 1570: loss 0.6515, time 373.67ms, mfu 6.29%\n",
      "iter 1580: loss 0.6516, time 367.48ms, mfu 6.33%\n",
      "iter 1590: loss 0.6637, time 373.40ms, mfu 6.35%\n",
      "iter 1600: loss 0.6731, time 372.68ms, mfu 6.37%\n",
      "iter 1610: loss 0.6632, time 364.24ms, mfu 6.41%\n",
      "iter 1620: loss 0.6627, time 368.69ms, mfu 6.43%\n",
      "iter 1630: loss 0.6437, time 364.64ms, mfu 6.46%\n",
      "iter 1640: loss 0.6303, time 365.19ms, mfu 6.49%\n",
      "iter 1650: loss 0.6305, time 366.38ms, mfu 6.51%\n",
      "iter 1660: loss 0.6280, time 368.67ms, mfu 6.52%\n",
      "iter 1670: loss 0.6347, time 368.40ms, mfu 6.54%\n",
      "iter 1680: loss 0.6521, time 367.27ms, mfu 6.55%\n",
      "iter 1690: loss 0.6324, time 374.06ms, mfu 6.55%\n",
      "iter 1700: loss 0.6506, time 370.39ms, mfu 6.56%\n",
      "iter 1710: loss 0.6278, time 365.10ms, mfu 6.57%\n",
      "iter 1720: loss 0.6247, time 365.51ms, mfu 6.59%\n",
      "iter 1730: loss 0.6160, time 367.02ms, mfu 6.59%\n",
      "iter 1740: loss 0.6514, time 367.35ms, mfu 6.60%\n",
      "step 1750: train loss 0.6046, val loss 0.6482\n",
      "saving checkpoint to out-kabyar-bpe\n",
      "iter 1750: loss 0.6217, time 12979.35ms, mfu 5.96%\n",
      "iter 1760: loss 0.6336, time 364.43ms, mfu 6.04%\n",
      "iter 1770: loss 0.6426, time 365.07ms, mfu 6.11%\n",
      "iter 1780: loss 0.6205, time 363.37ms, mfu 6.17%\n",
      "iter 1790: loss 0.6291, time 365.99ms, mfu 6.22%\n",
      "iter 1800: loss 0.6510, time 367.82ms, mfu 6.27%\n",
      "iter 1810: loss 0.6214, time 367.41ms, mfu 6.31%\n",
      "iter 1820: loss 0.6462, time 367.80ms, mfu 6.34%\n",
      "iter 1830: loss 0.6107, time 367.33ms, mfu 6.38%\n",
      "iter 1840: loss 0.6305, time 364.43ms, mfu 6.41%\n",
      "iter 1850: loss 0.6465, time 364.64ms, mfu 6.44%\n",
      "iter 1860: loss 0.6180, time 365.49ms, mfu 6.47%\n",
      "iter 1870: loss 0.6378, time 368.83ms, mfu 6.49%\n",
      "iter 1880: loss 0.5996, time 363.81ms, mfu 6.51%\n",
      "iter 1890: loss 0.6221, time 364.82ms, mfu 6.53%\n",
      "iter 1900: loss 0.6255, time 373.41ms, mfu 6.54%\n",
      "iter 1910: loss 0.6214, time 372.52ms, mfu 6.54%\n",
      "iter 1920: loss 0.6451, time 372.18ms, mfu 6.54%\n",
      "iter 1930: loss 0.6248, time 371.92ms, mfu 6.55%\n",
      "iter 1940: loss 0.6127, time 375.28ms, mfu 6.55%\n",
      "iter 1950: loss 0.6146, time 373.08ms, mfu 6.55%\n",
      "iter 1960: loss 0.6256, time 367.15ms, mfu 6.56%\n",
      "iter 1970: loss 0.6194, time 371.72ms, mfu 6.57%\n",
      "iter 1980: loss 0.6355, time 372.82ms, mfu 6.57%\n",
      "iter 1990: loss 0.6368, time 372.67ms, mfu 6.57%\n",
      "step 2000: train loss 0.5817, val loss 0.6420\n",
      "saving checkpoint to out-kabyar-bpe\n",
      "iter 2000: loss 0.5884, time 12989.71ms, mfu 5.93%\n",
      "iter 2010: loss 0.6109, time 369.18ms, mfu 6.00%\n",
      "iter 2020: loss 0.6217, time 370.64ms, mfu 6.06%\n",
      "iter 2030: loss 0.6287, time 373.37ms, mfu 6.11%\n",
      "iter 2040: loss 0.6115, time 365.01ms, mfu 6.17%\n",
      "iter 2050: loss 0.6152, time 365.77ms, mfu 6.23%\n",
      "iter 2060: loss 0.6418, time 363.90ms, mfu 6.28%\n",
      "iter 2070: loss 0.6100, time 367.36ms, mfu 6.32%\n",
      "iter 2080: loss 0.6008, time 372.43ms, mfu 6.34%\n",
      "iter 2090: loss 0.6011, time 368.70ms, mfu 6.37%\n",
      "iter 2100: loss 0.6085, time 370.61ms, mfu 6.40%\n",
      "iter 2110: loss 0.6187, time 363.84ms, mfu 6.43%\n",
      "iter 2120: loss 0.6031, time 363.97ms, mfu 6.46%\n",
      "iter 2130: loss 0.5947, time 365.53ms, mfu 6.49%\n",
      "iter 2140: loss 0.6222, time 365.49ms, mfu 6.51%\n",
      "iter 2150: loss 0.5941, time 369.00ms, mfu 6.52%\n",
      "iter 2160: loss 0.5771, time 367.64ms, mfu 6.54%\n",
      "iter 2170: loss 0.6148, time 363.91ms, mfu 6.56%\n",
      "iter 2180: loss 0.6127, time 366.02ms, mfu 6.57%\n",
      "iter 2190: loss 0.5984, time 366.57ms, mfu 6.58%\n",
      "iter 2200: loss 0.5950, time 364.81ms, mfu 6.60%\n",
      "iter 2210: loss 0.6310, time 366.61ms, mfu 6.60%\n",
      "iter 2220: loss 0.6142, time 367.11ms, mfu 6.61%\n",
      "iter 2230: loss 0.6027, time 365.87ms, mfu 6.62%\n",
      "iter 2240: loss 0.5883, time 369.04ms, mfu 6.62%\n",
      "step 2250: train loss 0.5624, val loss 0.6335\n",
      "saving checkpoint to out-kabyar-bpe\n",
      "iter 2250: loss 0.5980, time 12837.26ms, mfu 5.98%\n",
      "iter 2260: loss 0.5909, time 365.81ms, mfu 6.05%\n",
      "iter 2270: loss 0.5676, time 366.02ms, mfu 6.12%\n",
      "iter 2280: loss 0.6136, time 366.63ms, mfu 6.17%\n",
      "iter 2290: loss 0.5940, time 370.03ms, mfu 6.22%\n",
      "iter 2300: loss 0.5966, time 375.80ms, mfu 6.25%\n",
      "iter 2310: loss 0.5963, time 375.54ms, mfu 6.28%\n",
      "iter 2320: loss 0.5922, time 368.90ms, mfu 6.31%\n",
      "iter 2330: loss 0.5889, time 376.35ms, mfu 6.33%\n",
      "iter 2340: loss 0.5872, time 371.98ms, mfu 6.36%\n",
      "iter 2350: loss 0.5922, time 371.65ms, mfu 6.38%\n",
      "iter 2360: loss 0.5704, time 371.55ms, mfu 6.40%\n",
      "iter 2370: loss 0.5986, time 373.90ms, mfu 6.42%\n",
      "iter 2380: loss 0.5886, time 366.58ms, mfu 6.45%\n",
      "iter 2390: loss 0.5950, time 364.70ms, mfu 6.47%\n",
      "iter 2400: loss 0.5872, time 367.24ms, mfu 6.49%\n",
      "iter 2410: loss 0.5710, time 367.09ms, mfu 6.51%\n",
      "iter 2420: loss 0.5990, time 369.05ms, mfu 6.52%\n",
      "iter 2430: loss 0.5923, time 369.65ms, mfu 6.54%\n",
      "iter 2440: loss 0.5921, time 369.94ms, mfu 6.54%\n",
      "iter 2450: loss 0.5959, time 368.64ms, mfu 6.55%\n",
      "iter 2460: loss 0.5947, time 366.28ms, mfu 6.57%\n",
      "iter 2470: loss 0.5520, time 364.50ms, mfu 6.58%\n",
      "iter 2480: loss 0.5736, time 365.83ms, mfu 6.60%\n",
      "iter 2490: loss 0.5874, time 366.96ms, mfu 6.60%\n",
      "step 2500: train loss 0.5437, val loss 0.6302\n",
      "saving checkpoint to out-kabyar-bpe\n",
      "iter 2500: loss 0.5605, time 12964.87ms, mfu 5.96%\n",
      "iter 2510: loss 0.5801, time 365.39ms, mfu 6.04%\n",
      "iter 2520: loss 0.5661, time 366.59ms, mfu 6.10%\n",
      "iter 2530: loss 0.5751, time 366.63ms, mfu 6.16%\n",
      "iter 2540: loss 0.5763, time 365.64ms, mfu 6.21%\n",
      "iter 2550: loss 0.5790, time 371.45ms, mfu 6.25%\n",
      "iter 2560: loss 0.5773, time 369.14ms, mfu 6.29%\n",
      "iter 2570: loss 0.5293, time 368.46ms, mfu 6.33%\n",
      "iter 2580: loss 0.5722, time 365.81ms, mfu 6.37%\n",
      "iter 2590: loss 0.5385, time 364.16ms, mfu 6.40%\n",
      "iter 2600: loss 0.5741, time 365.43ms, mfu 6.43%\n",
      "iter 2610: loss 0.5573, time 366.16ms, mfu 6.46%\n",
      "iter 2620: loss 0.5585, time 368.91ms, mfu 6.48%\n",
      "iter 2630: loss 0.5855, time 366.73ms, mfu 6.50%\n",
      "iter 2640: loss 0.5689, time 364.80ms, mfu 6.52%\n",
      "iter 2650: loss 0.5527, time 364.61ms, mfu 6.54%\n",
      "iter 2660: loss 0.5833, time 366.67ms, mfu 6.55%\n",
      "iter 2670: loss 0.5952, time 368.80ms, mfu 6.56%\n",
      "iter 2680: loss 0.5608, time 369.53ms, mfu 6.57%\n",
      "iter 2690: loss 0.5728, time 369.52ms, mfu 6.58%\n",
      "iter 2700: loss 0.5693, time 371.31ms, mfu 6.58%\n",
      "iter 2710: loss 0.5575, time 370.29ms, mfu 6.58%\n",
      "iter 2720: loss 0.5822, time 369.43ms, mfu 6.59%\n",
      "iter 2730: loss 0.5349, time 363.64ms, mfu 6.60%\n",
      "iter 2740: loss 0.5632, time 364.14ms, mfu 6.62%\n",
      "step 2750: train loss 0.5270, val loss 0.6261\n",
      "saving checkpoint to out-kabyar-bpe\n",
      "iter 2750: loss 0.5752, time 12996.86ms, mfu 5.97%\n",
      "iter 2760: loss 0.5683, time 363.87ms, mfu 6.05%\n",
      "iter 2770: loss 0.5685, time 366.56ms, mfu 6.11%\n",
      "iter 2780: loss 0.5541, time 365.47ms, mfu 6.17%\n",
      "iter 2790: loss 0.5497, time 369.57ms, mfu 6.22%\n",
      "iter 2800: loss 0.5539, time 368.93ms, mfu 6.26%\n",
      "iter 2810: loss 0.5542, time 368.74ms, mfu 6.30%\n",
      "iter 2820: loss 0.5368, time 371.47ms, mfu 6.33%\n",
      "iter 2830: loss 0.5398, time 371.23ms, mfu 6.36%\n",
      "iter 2840: loss 0.5457, time 366.95ms, mfu 6.39%\n",
      "iter 2850: loss 0.5573, time 364.55ms, mfu 6.42%\n",
      "iter 2860: loss 0.5423, time 367.07ms, mfu 6.45%\n",
      "iter 2870: loss 0.5791, time 366.96ms, mfu 6.47%\n",
      "iter 2880: loss 0.5766, time 367.62ms, mfu 6.49%\n",
      "iter 2890: loss 0.5764, time 374.60ms, mfu 6.50%\n",
      "iter 2900: loss 0.5566, time 371.95ms, mfu 6.51%\n",
      "iter 2910: loss 0.5907, time 372.66ms, mfu 6.51%\n",
      "iter 2920: loss 0.5375, time 369.28ms, mfu 6.53%\n",
      "iter 2930: loss 0.5345, time 366.36ms, mfu 6.54%\n",
      "iter 2940: loss 0.5530, time 367.28ms, mfu 6.56%\n",
      "iter 2950: loss 0.5426, time 370.10ms, mfu 6.56%\n",
      "iter 2960: loss 0.5467, time 370.23ms, mfu 6.57%\n",
      "iter 2970: loss 0.5875, time 368.70ms, mfu 6.58%\n",
      "iter 2980: loss 0.5490, time 363.75ms, mfu 6.59%\n",
      "iter 2990: loss 0.5383, time 364.09ms, mfu 6.61%\n",
      "step 3000: train loss 0.5109, val loss 0.6317\n",
      "iter 3000: loss 0.5586, time 10971.63ms, mfu 5.97%\n",
      "iter 3010: loss 0.5528, time 365.79ms, mfu 6.04%\n",
      "iter 3020: loss 0.5639, time 364.76ms, mfu 6.11%\n",
      "iter 3030: loss 0.5801, time 366.56ms, mfu 6.17%\n",
      "iter 3040: loss 0.5609, time 365.86ms, mfu 6.22%\n",
      "iter 3050: loss 0.5375, time 365.26ms, mfu 6.27%\n",
      "iter 3060: loss 0.5290, time 367.46ms, mfu 6.31%\n",
      "iter 3070: loss 0.5575, time 365.65ms, mfu 6.35%\n",
      "iter 3080: loss 0.5511, time 363.31ms, mfu 6.39%\n",
      "iter 3090: loss 0.5690, time 366.99ms, mfu 6.42%\n",
      "iter 3100: loss 0.5307, time 366.23ms, mfu 6.45%\n",
      "iter 3110: loss 0.5259, time 367.42ms, mfu 6.47%\n",
      "iter 3120: loss 0.5361, time 371.59ms, mfu 6.48%\n",
      "iter 3130: loss 0.5493, time 374.29ms, mfu 6.49%\n",
      "iter 3140: loss 0.5516, time 364.79ms, mfu 6.51%\n",
      "iter 3150: loss 0.5372, time 367.75ms, mfu 6.53%\n",
      "iter 3160: loss 0.5472, time 363.81ms, mfu 6.55%\n",
      "iter 3170: loss 0.5329, time 365.95ms, mfu 6.56%\n",
      "iter 3180: loss 0.5264, time 369.87ms, mfu 6.57%\n",
      "iter 3190: loss 0.5463, time 370.10ms, mfu 6.57%\n",
      "iter 3200: loss 0.5335, time 370.51ms, mfu 6.58%\n",
      "iter 3210: loss 0.5346, time 366.76ms, mfu 6.59%\n",
      "iter 3220: loss 0.5473, time 365.69ms, mfu 6.60%\n",
      "iter 3230: loss 0.5380, time 363.93ms, mfu 6.61%\n",
      "iter 3240: loss 0.5515, time 370.42ms, mfu 6.61%\n",
      "step 3250: train loss 0.4949, val loss 0.6210\n",
      "saving checkpoint to out-kabyar-bpe\n",
      "iter 3250: loss 0.5259, time 12908.96ms, mfu 5.97%\n",
      "iter 3260: loss 0.5463, time 365.29ms, mfu 6.05%\n",
      "iter 3270: loss 0.5171, time 365.57ms, mfu 6.11%\n",
      "iter 3280: loss 0.5389, time 367.88ms, mfu 6.17%\n",
      "iter 3290: loss 0.5345, time 371.05ms, mfu 6.21%\n",
      "iter 3300: loss 0.5314, time 372.18ms, mfu 6.25%\n",
      "iter 3310: loss 0.5391, time 372.94ms, mfu 6.28%\n",
      "iter 3320: loss 0.5162, time 373.62ms, mfu 6.31%\n",
      "iter 3330: loss 0.5284, time 373.13ms, mfu 6.33%\n",
      "iter 3340: loss 0.5551, time 374.88ms, mfu 6.35%\n",
      "iter 3350: loss 0.5247, time 372.71ms, mfu 6.38%\n",
      "iter 3360: loss 0.5111, time 375.05ms, mfu 6.39%\n",
      "iter 3370: loss 0.5290, time 366.22ms, mfu 6.42%\n",
      "iter 3380: loss 0.5409, time 365.38ms, mfu 6.45%\n",
      "iter 3390: loss 0.5566, time 365.70ms, mfu 6.48%\n",
      "iter 3400: loss 0.5456, time 371.04ms, mfu 6.49%\n",
      "iter 3410: loss 0.5229, time 369.16ms, mfu 6.50%\n",
      "iter 3420: loss 0.5429, time 371.18ms, mfu 6.51%\n",
      "iter 3430: loss 0.5042, time 364.79ms, mfu 6.54%\n",
      "iter 3440: loss 0.5371, time 365.98ms, mfu 6.55%\n",
      "iter 3450: loss 0.5033, time 364.55ms, mfu 6.57%\n",
      "iter 3460: loss 0.5140, time 365.01ms, mfu 6.58%\n",
      "iter 3470: loss 0.5357, time 364.41ms, mfu 6.60%\n",
      "iter 3480: loss 0.5123, time 368.50ms, mfu 6.60%\n",
      "iter 3490: loss 0.4929, time 366.00ms, mfu 6.61%\n",
      "step 3500: train loss 0.4809, val loss 0.6260\n",
      "iter 3500: loss 0.5301, time 11090.42ms, mfu 5.97%\n",
      "iter 3510: loss 0.5285, time 372.85ms, mfu 6.03%\n",
      "iter 3520: loss 0.5425, time 373.61ms, mfu 6.09%\n",
      "iter 3530: loss 0.5451, time 372.79ms, mfu 6.13%\n",
      "iter 3540: loss 0.5242, time 368.34ms, mfu 6.19%\n",
      "iter 3550: loss 0.5310, time 375.63ms, mfu 6.22%\n",
      "iter 3560: loss 0.5136, time 375.90ms, mfu 6.25%\n",
      "iter 3570: loss 0.5022, time 372.23ms, mfu 6.28%\n",
      "iter 3580: loss 0.5037, time 375.54ms, mfu 6.31%\n",
      "iter 3590: loss 0.5165, time 374.95ms, mfu 6.33%\n",
      "iter 3600: loss 0.5403, time 372.86ms, mfu 6.36%\n",
      "iter 3610: loss 0.5125, time 371.02ms, mfu 6.38%\n",
      "iter 3620: loss 0.5105, time 372.56ms, mfu 6.40%\n",
      "iter 3630: loss 0.4908, time 369.67ms, mfu 6.42%\n",
      "iter 3640: loss 0.5276, time 370.58ms, mfu 6.44%\n",
      "iter 3650: loss 0.5097, time 374.21ms, mfu 6.45%\n",
      "iter 3660: loss 0.5318, time 375.24ms, mfu 6.46%\n",
      "iter 3670: loss 0.5241, time 374.07ms, mfu 6.47%\n",
      "iter 3680: loss 0.5062, time 367.96ms, mfu 6.49%\n",
      "iter 3690: loss 0.5301, time 367.54ms, mfu 6.51%\n",
      "iter 3700: loss 0.5121, time 373.73ms, mfu 6.51%\n",
      "iter 3710: loss 0.5290, time 364.34ms, mfu 6.53%\n",
      "iter 3720: loss 0.4970, time 364.07ms, mfu 6.55%\n",
      "iter 3730: loss 0.5151, time 364.49ms, mfu 6.57%\n",
      "iter 3740: loss 0.5020, time 368.08ms, mfu 6.58%\n",
      "step 3750: train loss 0.4675, val loss 0.6264\n",
      "iter 3750: loss 0.5267, time 11050.75ms, mfu 5.94%\n",
      "iter 3760: loss 0.5136, time 372.24ms, mfu 6.01%\n",
      "iter 3770: loss 0.5086, time 372.78ms, mfu 6.06%\n",
      "iter 3780: loss 0.5071, time 371.57ms, mfu 6.12%\n",
      "iter 3790: loss 0.5184, time 370.78ms, mfu 6.17%\n",
      "iter 3800: loss 0.5097, time 371.48ms, mfu 6.21%\n",
      "iter 3810: loss 0.5260, time 369.36ms, mfu 6.25%\n",
      "iter 3820: loss 0.4896, time 375.09ms, mfu 6.28%\n",
      "iter 3830: loss 0.4883, time 374.56ms, mfu 6.31%\n",
      "iter 3840: loss 0.5034, time 372.55ms, mfu 6.33%\n",
      "iter 3850: loss 0.5225, time 374.02ms, mfu 6.36%\n",
      "iter 3860: loss 0.4944, time 373.24ms, mfu 6.38%\n",
      "iter 3870: loss 0.4960, time 373.06ms, mfu 6.40%\n",
      "iter 3880: loss 0.5287, time 375.05ms, mfu 6.41%\n",
      "iter 3890: loss 0.5251, time 370.23ms, mfu 6.43%\n",
      "iter 3900: loss 0.4844, time 375.27ms, mfu 6.44%\n",
      "iter 3910: loss 0.5242, time 366.05ms, mfu 6.47%\n",
      "iter 3920: loss 0.5146, time 374.81ms, mfu 6.47%\n",
      "iter 3930: loss 0.5224, time 371.27ms, mfu 6.49%\n",
      "iter 3940: loss 0.4990, time 376.67ms, mfu 6.49%\n",
      "iter 3950: loss 0.5142, time 371.52ms, mfu 6.50%\n",
      "iter 3960: loss 0.4913, time 367.65ms, mfu 6.52%\n",
      "iter 3970: loss 0.5235, time 373.11ms, mfu 6.52%\n",
      "iter 3980: loss 0.4922, time 372.89ms, mfu 6.53%\n",
      "iter 3990: loss 0.4935, time 374.01ms, mfu 6.53%\n",
      "step 4000: train loss 0.4571, val loss 0.6252\n",
      "iter 4000: loss 0.4926, time 11088.10ms, mfu 5.90%\n",
      "iter 4010: loss 0.4856, time 371.15ms, mfu 5.97%\n",
      "iter 4020: loss 0.5221, time 375.27ms, mfu 6.03%\n",
      "iter 4030: loss 0.4932, time 374.43ms, mfu 6.08%\n",
      "iter 4040: loss 0.4930, time 374.04ms, mfu 6.13%\n",
      "iter 4050: loss 0.4738, time 374.28ms, mfu 6.17%\n",
      "iter 4060: loss 0.4849, time 374.20ms, mfu 6.21%\n",
      "iter 4070: loss 0.4880, time 374.75ms, mfu 6.24%\n",
      "iter 4080: loss 0.5072, time 372.16ms, mfu 6.27%\n",
      "iter 4090: loss 0.4844, time 373.33ms, mfu 6.30%\n",
      "iter 4100: loss 0.5015, time 371.75ms, mfu 6.33%\n",
      "iter 4110: loss 0.5101, time 375.09ms, mfu 6.35%\n",
      "iter 4120: loss 0.5046, time 373.24ms, mfu 6.37%\n",
      "iter 4130: loss 0.4830, time 373.94ms, mfu 6.39%\n",
      "iter 4140: loss 0.4913, time 373.62ms, mfu 6.41%\n",
      "iter 4150: loss 0.5102, time 371.87ms, mfu 6.43%\n",
      "iter 4160: loss 0.5099, time 375.08ms, mfu 6.44%\n",
      "iter 4170: loss 0.5060, time 372.54ms, mfu 6.45%\n",
      "iter 4180: loss 0.5234, time 372.05ms, mfu 6.47%\n",
      "iter 4190: loss 0.5170, time 373.47ms, mfu 6.48%\n",
      "iter 4200: loss 0.5065, time 371.33ms, mfu 6.49%\n",
      "iter 4210: loss 0.5255, time 372.62ms, mfu 6.50%\n",
      "iter 4220: loss 0.4898, time 375.55ms, mfu 6.50%\n",
      "iter 4230: loss 0.5055, time 372.44ms, mfu 6.51%\n",
      "iter 4240: loss 0.4979, time 374.05ms, mfu 6.51%\n",
      "step 4250: train loss 0.4478, val loss 0.6315\n",
      "iter 4250: loss 0.4817, time 11087.76ms, mfu 5.88%\n",
      "iter 4260: loss 0.4858, time 370.83ms, mfu 5.96%\n",
      "iter 4270: loss 0.5157, time 374.46ms, mfu 6.02%\n",
      "iter 4280: loss 0.5058, time 370.58ms, mfu 6.08%\n",
      "iter 4290: loss 0.5155, time 373.69ms, mfu 6.12%\n",
      "iter 4300: loss 0.4836, time 369.35ms, mfu 6.17%\n",
      "iter 4310: loss 0.4829, time 375.12ms, mfu 6.21%\n",
      "iter 4320: loss 0.5046, time 372.27ms, mfu 6.25%\n",
      "iter 4330: loss 0.4898, time 371.71ms, mfu 6.28%\n",
      "iter 4340: loss 0.5178, time 376.85ms, mfu 6.30%\n",
      "iter 4350: loss 0.4734, time 375.02ms, mfu 6.33%\n",
      "iter 4360: loss 0.4835, time 372.69ms, mfu 6.35%\n",
      "iter 4370: loss 0.5053, time 373.62ms, mfu 6.37%\n",
      "iter 4380: loss 0.4849, time 373.12ms, mfu 6.39%\n",
      "iter 4390: loss 0.5062, time 374.27ms, mfu 6.41%\n",
      "iter 4400: loss 0.4909, time 373.57ms, mfu 6.42%\n",
      "iter 4410: loss 0.4986, time 373.78ms, mfu 6.44%\n",
      "iter 4420: loss 0.4841, time 373.38ms, mfu 6.45%\n",
      "iter 4430: loss 0.4651, time 370.92ms, mfu 6.47%\n",
      "iter 4440: loss 0.5001, time 373.48ms, mfu 6.48%\n",
      "iter 4450: loss 0.4870, time 372.38ms, mfu 6.49%\n",
      "iter 4460: loss 0.4979, time 371.10ms, mfu 6.50%\n",
      "iter 4470: loss 0.4770, time 375.06ms, mfu 6.50%\n",
      "iter 4480: loss 0.5046, time 371.32ms, mfu 6.51%\n",
      "iter 4490: loss 0.4846, time 374.37ms, mfu 6.52%\n",
      "step 4500: train loss 0.4386, val loss 0.6313\n",
      "iter 4500: loss 0.4641, time 11087.50ms, mfu 5.89%\n",
      "iter 4510: loss 0.4782, time 370.19ms, mfu 5.96%\n",
      "iter 4520: loss 0.4931, time 374.96ms, mfu 6.02%\n",
      "iter 4530: loss 0.4956, time 370.69ms, mfu 6.08%\n",
      "iter 4540: loss 0.4908, time 371.28ms, mfu 6.13%\n",
      "iter 4550: loss 0.4996, time 375.67ms, mfu 6.17%\n",
      "iter 4560: loss 0.4795, time 370.13ms, mfu 6.21%\n",
      "iter 4570: loss 0.4831, time 370.98ms, mfu 6.25%\n",
      "iter 4580: loss 0.5017, time 372.22ms, mfu 6.29%\n",
      "iter 4590: loss 0.4766, time 369.58ms, mfu 6.32%\n",
      "iter 4600: loss 0.4850, time 371.94ms, mfu 6.35%\n",
      "iter 4610: loss 0.5004, time 371.36ms, mfu 6.37%\n",
      "iter 4620: loss 0.4991, time 377.17ms, mfu 6.39%\n",
      "iter 4630: loss 0.5121, time 375.52ms, mfu 6.40%\n",
      "iter 4640: loss 0.4762, time 367.61ms, mfu 6.43%\n",
      "iter 4650: loss 0.4953, time 374.63ms, mfu 6.44%\n",
      "iter 4660: loss 0.4754, time 373.71ms, mfu 6.45%\n",
      "iter 4670: loss 0.4749, time 371.63ms, mfu 6.46%\n",
      "iter 4680: loss 0.4752, time 368.31ms, mfu 6.48%\n",
      "iter 4690: loss 0.4998, time 367.40ms, mfu 6.50%\n",
      "iter 4700: loss 0.4799, time 373.47ms, mfu 6.51%\n",
      "iter 4710: loss 0.4842, time 373.65ms, mfu 6.51%\n",
      "iter 4720: loss 0.5092, time 372.38ms, mfu 6.52%\n",
      "iter 4730: loss 0.4732, time 371.25ms, mfu 6.53%\n",
      "iter 4740: loss 0.5275, time 373.13ms, mfu 6.53%\n",
      "step 4750: train loss 0.4319, val loss 0.6326\n",
      "iter 4750: loss 0.4792, time 11091.91ms, mfu 5.90%\n",
      "iter 4760: loss 0.4838, time 371.97ms, mfu 5.97%\n",
      "iter 4770: loss 0.4600, time 373.30ms, mfu 6.03%\n",
      "iter 4780: loss 0.5059, time 371.27ms, mfu 6.09%\n",
      "iter 4790: loss 0.4869, time 373.98ms, mfu 6.13%\n",
      "iter 4800: loss 0.4930, time 371.51ms, mfu 6.18%\n",
      "iter 4810: loss 0.4767, time 375.47ms, mfu 6.22%\n",
      "iter 4820: loss 0.4629, time 374.74ms, mfu 6.25%\n",
      "iter 4830: loss 0.4902, time 371.39ms, mfu 6.28%\n",
      "iter 4840: loss 0.4786, time 375.55ms, mfu 6.31%\n",
      "iter 4850: loss 0.4893, time 373.83ms, mfu 6.33%\n",
      "iter 4860: loss 0.4791, time 371.42ms, mfu 6.36%\n",
      "iter 4870: loss 0.4760, time 376.07ms, mfu 6.37%\n",
      "iter 4880: loss 0.4868, time 373.25ms, mfu 6.39%\n",
      "iter 4890: loss 0.4814, time 375.83ms, mfu 6.41%\n",
      "iter 4900: loss 0.5171, time 373.06ms, mfu 6.42%\n",
      "iter 4910: loss 0.4927, time 368.85ms, mfu 6.45%\n",
      "iter 4920: loss 0.4829, time 373.68ms, mfu 6.46%\n",
      "iter 4930: loss 0.4783, time 374.16ms, mfu 6.47%\n",
      "iter 4940: loss 0.4898, time 373.63ms, mfu 6.48%\n",
      "iter 4950: loss 0.4886, time 372.12ms, mfu 6.49%\n",
      "iter 4960: loss 0.4743, time 371.39ms, mfu 6.50%\n",
      "iter 4970: loss 0.4985, time 372.17ms, mfu 6.51%\n",
      "iter 4980: loss 0.4849, time 370.42ms, mfu 6.52%\n",
      "iter 4990: loss 0.4931, time 371.31ms, mfu 6.53%\n",
      "step 5000: train loss 0.4265, val loss 0.6374\n",
      "iter 5000: loss 0.4706, time 11089.18ms, mfu 5.90%\n",
      "\n",
      "real\t35m18.770s\n",
      "user\t34m52.487s\n",
      "sys\t0m15.616s\n"
     ]
    }
   ],
   "source": [
    "!time python -m torch.distributed.launch --use-env train.py ./config/train_kabyar_bpe.py | tee train-kabyar-bpe.log1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Testing-1 with Kabyar Corpus 1.0 BPE Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rnd/anaconda3/envs/nanoGPT/lib/python3.8/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use-env is set by default in torchrun.\n",
      "If your script expects `--local-rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  warnings.warn(\n",
      "Overriding: out_dir = out-kabyar-bpe\n",
      "number of parameters: 29.94M\n",
      "No meta.pkl found, assuming GPT-2 encodings...\n",
      "\n",
      "မင်းတို့ရဲ့ ဆောက်ကိုက်နေတဲ့\n",
      "ဟောင်းလေးတွေလို\n",
      "တစ်နေ့တစ်ခါ နှစ်ခုခုလုံးက\n",
      "ကိုယ့်အတွက်ပျက် ရွှေလက်နေတဲ့ များတယ်လို့\n",
      "အချစ်တွေ များတယ်လို့ များတယ်\n",
      "များတယ်\n",
      "မျှားတယ်\n",
      "များတယ် မျဉ်းတယ်\n",
      "မျှော\n",
      "---------------\n",
      "\n",
      "ကျေးဇူးစကား တို့တန်ခူးလေး ။\n",
      "ကျေးဇူးငွေ့ မုန်တိုင်းလေး ၊ တေးချိုချိုနဲ့ ။\n",
      "ဘီယိုက်တို့ မွေးရပ်ရေး ၊ အမေးမြအေးကိုလည်း ။\n",
      "မေးကွေးမေးကြောင်း ၊ မောင်းမဲ့လေးကလေး ။\n",
      "မဗေဒါ မယ့်အတိတ်ကလေး ၊ လေပြည်သ�\n",
      "---------------\n",
      "\n",
      "ပြာလို့သွားတဲ့ ချောင်းရောက်ကြောင့်\n",
      "မြောက်နှင်းများတဲ့ မြောက်ကမ်းကြားလေ . . .\n",
      "တိတ်ဆိတ်တဲ့ စိတ်ရစ်ခုပဲ ကောင်းမှုအထိ ဖန်တီးခြင်း . . .\n",
      "တိတ်ရိတ်ရဲ့ဘက်ပါ\n",
      "မြက်ခင်းပေါ်မှာ ဝါတို့ကို ငါတို့ အနက်မ�\n",
      "---------------\n",
      "\n",
      "အဆိပ်ကို ဖုံးလို့လန်း ။\n",
      "ကြံစည်ပန်းခွင် ပျိုးကိုလေး ။\n",
      "\n",
      "Title: ဥဒါနှင့် အမေ\n",
      "By: ဇာနည်\n",
      "အမောင်ချောင်း မောင်ချိုတောင်း၊\n",
      "လူအိုရှင် လူလတာရာလေး၊\n",
      "သူ့ဓလေ့ရယ်နှင့်\n",
      "သင်ယောင်မမြင် ရွှေရည်တော်တွင်၊\n",
      "ရင်မှာ�\n",
      "---------------\n",
      "\n",
      "အမြိုက်တွေ များများတယ်\n",
      "စိတ်လေး မာန်ကျောင်းသားရေးဆိုတဲ့အခါ\n",
      "ငါ့မှာ စပယ်မှာ အမြိုက်နဲ့ ဆင်ခြင်တွေရဲ့ အဆင့်ပြင်အစှိုက်တွေ\n",
      "အခု အလိုက်စီးတဲ့ တစ်ခုတည်းတည်း သိမ်းကြောက်ရင်း\n",
      "ကြိုးတာကို ဓား�\n",
      "---------------\n",
      "\n",
      "ငါက သေသေတာကို သာသနာလို့ ထမ်းဆေးမှူးကြ\n",
      "သွေးကို အသံပေးကြပါရစေ\n",
      "တိုင်းပြည်တစ်ခုရဲ ငါတို့အတွက်\n",
      "အမြတ်ဆုံးမရှုံးရင် မျက်နှာဖုံးလင်းထည်သွားတဲ့ တိုင်းပြည်\n",
      "ဆ - မိုးကုန် ချစ်သောအခါ ဗိုလ်ချုပ်ရ\n",
      "---------------\n",
      "\n",
      "လေပြည်ဝင်းလေးနဲ့၊ ပန်းလွင့်မွှေးစို့\n",
      "အိုးလေးတို့မြေ၊ လူကလေးနဲ့\n",
      "ဘဝလာရောင်၊ မြေမှာတောင်မည်\n",
      "မရေပြည့်သောင်းရန်၊ လူထက်တန်လှန်သည် ။\n",
      "အိုင်ခွန်အားကို၊ အသီးမည်မူ\n",
      "လွမ်းမိုရ်ချမ်းမြ တင့်တယ\n",
      "---------------\n",
      "\n",
      "အံ့ချီးသည့်\n",
      "နွဲ့မြီးမှည့်ရမည် ။\n",
      "တစ်နွေကို တစ်ရာမခံနိုင်တော့\n",
      "အပြုံးတစ်များ စတင်ပေါက်ပါ\n",
      "နှုတ်ခမ်းရောင်စုံ အစဉ်တားစုံနှင့်\n",
      "ကြိုးစားစိတ်ကြိမ် သိပါစေလေ\n",
      "မဖြစ်လောကြောင့် မယ်မယ်မထင်ရှာ�\n",
      "---------------\n",
      "\n",
      "ကြိုတင်းကောင်းလာ ချိုမြဲမြဲသည်\n",
      "သူတို့ဆွေအား တောင်ပံတော်၏\n",
      "များများငဲ့ကျောင်း များခဲ့သော်\n",
      "ကိုယ်ကြည့်စမ်းပါ ။\n",
      "ကိုယ့်မျက်လုံးများပင်\n",
      "အစားရှိပါတယ ဖိတ်မည်လို့ရယ် ။\n",
      "အမှောင်ထဲမှာ\n",
      "ကျောက်�\n",
      "---------------\n",
      "\n",
      "အဖေနဲ့ ကြွက်လပ်တွေ\n",
      "ကျနော် ရွှေရင် ဘယ်ဝယ်လို့ လိုပါလား\n",
      "ကျနော် လည်ပတ်လေးတွေကို\n",
      "မော် မေ့ချင်တော့ရှောက်တယ် ။\n",
      "ကျနော် ခင်ဗျားတော့ ပြုတ်ကျနေမယ် ။\n",
      "တစ်နေ့တစ်နေ့ကတော့\n",
      "ဗျောကြီးတစ်ပွင့်တစ်ခုရဲ�\n",
      "---------------\n",
      "\n",
      "real\t0m26.663s\n",
      "user\t0m20.732s\n",
      "sys\t0m5.368s\n"
     ]
    }
   ],
   "source": [
    "!time python -m torch.distributed.launch --use-env sample.py --out_dir=out-kabyar-bpe | tee test_kabyar_bpe.dist.log1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing-2 with Kabyar Corpus 1.0 with BPE Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rnd/anaconda3/envs/nanoGPT/lib/python3.8/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use-env is set by default in torchrun.\n",
      "If your script expects `--local-rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  warnings.warn(\n",
      "Overriding: out_dir = out-kabyar-bpe\n",
      "number of parameters: 29.94M\n",
      "No meta.pkl found, assuming GPT-2 encodings...\n",
      "\n",
      "နွေသည်နေ့၏ ဆိုင်းကစားသော်ကြောင့်\n",
      "သင် အဖြေများကြောင့်\n",
      "ကြောက်ကြောင့်ပင် နှစ်လေးတစ်ပင်တစ်ပွင့်\n",
      "တစ်ပင် ရေခြင်းသားတစ်ပွင့်\n",
      "စေတနာမှန်မှု ကောက်ပင်လယ်ပြင်ပြင်\n",
      "ကျောင်းသားမငြီး သူယှဉ်လမ်\n",
      "---------------\n",
      "\n",
      "သူတို့တစ်ရပ်ရဲ့ ကြမ်းမားပါ\n",
      "အားလုံးကို အသစ်တစ်ခုလုံးကို လုံခြုံဖြတ်\n",
      "ရိပ်လုံးလေးကို ကူးခတ်လိုက် ပြန်မလိုက်ဘူး\n",
      "လူတော်ကောင်းလေးရေ ခန္ဓာကိုယ်တွေ မရှက်ဘူး\n",
      "မီးပွက်လဲပြီး ရေးချင်းချင်းက�\n",
      "---------------\n",
      "\n",
      "ပေါင်းသင်းမြက်ကြား ၊ စုံပြီးပါဟု\n",
      "ရေလွှာဖွယ်ရာ ၊ ကျွန့်မြော်ခါမှ\n",
      "ဖြားပညာရှာ ၊ စာကလေးငယ်နှင့်\n",
      "ပတ်ကြားလှသည်နှင့် ၊ အတွေးလျှံတည့် ။\n",
      "အကျွန်ုပ်ကိုယ်စီ ၊ တုံ့ပြည်မြိုင်ညို\n",
      "လူရိုင်းဘွဲ့ဝင် ၊\n",
      "---------------\n",
      "\n",
      "ပွင့်လွှားမှုဝပ် ၊ အပြီလျှံသည်\n",
      "ရောင်တွင်းချမ်းအေး ၊ ချိုတေးလှမ်းသည်\n",
      "ချမ်းမြေ့ဂုဏ်ရည် လှောင်ရီမှာ ။\n",
      "စိတ်အစာကုန် ၊ သူမဆုံးလည်း\n",
      "မွေးဖွားတောင်လို့ ၊ တောင်မြင်ပို့သား ။\n",
      "ချောင်းပေါက်မှာတေ\n",
      "---------------\n",
      "\n",
      "တစ်ခါတလေ ဒီလိုပဲ။\n",
      "အခါက ဆုံးခဲ့တစ်ခါ မီဆိုင်ကို မီးရှိတယ် လိုအပ်တယ်။\n",
      "ကိုယ်က ကျုပ်ကျဆုံးတော့ ကိုယ့်နင်းကို မင် မအေးခဲ့တာ\n",
      "စိတ်ရုံက ပြန်ကောင်းမှုတွေ ဘယ်ကို ဖြောင့်တယ်\n",
      "ကိုယ့်နင့်ကို ဖြစ်မ�\n",
      "---------------\n",
      "\n",
      "ဒီလှိုင်းမှာ ပြောင်းပါတယ်\n",
      "မင်းထဲမှာ ဘာကြောင့်လဲ လာလာတဲ့မယားတွေကို ဘာတွေ ခြေရာခက်လို့ မရတော့ဘူးပါ\n",
      "မင်းရဲ့ ဆက်စက်ဝိုင်းတွေကို ထမ်းလို့သွားမှာပါ မရှိဘူး။\n",
      "မနက်ဖြန်အတွက် ပြောရရင်\n",
      "ရှိတယ�\n",
      "---------------\n",
      "\n",
      "မင်းမြတ်တတ်လတ် တစ်ချပ်ရာစတို့မှ\n",
      "စိတ်ထားကာလတွင် လောင်းသတင်းရင်းသားပေါ်က လေ့လာ\n",
      "မစ္ဆာစိုးဘဲ တားမမေ့ပေးနိုင်တဲ့\n",
      "ရောင်နီနီတွေလား ရောက်ရှိတယ်\n",
      "မြောက်ဘက်ဘက်ရောက်ပြီကွာလာတယ်\n",
      "မြောက်တစ်ခ\n",
      "---------------\n",
      "\n",
      "ချောင်းရောင်းလာလိုက်တာကို အကြပ်မထားဘူး\n",
      "သူမြတ်နိုးတဲ့ ကျီးလန့်ကျောင်းကန်ချီခဲ့တဲ့ အမည်မဲ့လမ်းကို ပြန့်ကျဲသွားခဲ့ရတဲ့အခါ\n",
      "မရောက်သလားတဲ့ မျက်ရည်တွေ ပြောင်းထွေးတယ်\n",
      "မျှော်လင့်ချက်တ�\n",
      "---------------\n",
      "\n",
      "By: မင်းသုဝဏ်\n",
      "မောဟမြိုင်မြိုင် အတ္တတွေကိုလည်း\n",
      "သင်မဏ္ဍိုင်း သည်းမြိုင်မှုန်းလည်း\n",
      "အောင်မြင်ချိုမြ မြန်မာ့ဓလေ့တည်း\n",
      "စွမ်းအားရုံမြောက် နေခြည်တောက်ကြီးတာ\n",
      "စောင့်ရှောက်များများကြောင့် ခပ�\n",
      "---------------\n",
      "\n",
      "အဖြေရှာလိမ့်မယ် ။\n",
      "မိုးတိမ်တို့လို့ အဖြေသာကူးခဲ့တဲ့\n",
      "ငါ့ကိုယ်က အမြီးတရားထိပ်ဟာ\n",
      "ဝေဒနာဆိုတဲ့ စာကိုင်းစိတ်ဆိတ် မျက်ရည်စိတ်ဝမ်းနေကြတယ်\n",
      "အမြဲတမ်း တရားရှိတယ်\n",
      "အဲဒီနောက်မောင်းဆိုတာ မီးရေအိ\n",
      "---------------\n",
      "\n",
      "real\t0m26.669s\n",
      "user\t0m20.499s\n",
      "sys\t0m5.512s\n"
     ]
    }
   ],
   "source": [
    "!time python -m torch.distributed.launch --use-env sample.py --out_dir=out-kabyar-bpe | tee test_kabyar_bpe.dist.log2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: BPE unit နဲ့ sub-word level segmentation လုပ်ပြီး train လုပ်တဲ့အခါမှာတော့ Unknown character လိုမျိုး ပေါ်လာတာကိုတော့ တွေ့ရတယ်။  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "[1] https://github.com/karpathy/nanoGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
